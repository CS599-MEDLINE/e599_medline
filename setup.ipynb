{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI E-599 Setup of *PubMiner* backend application\n",
    "=====\n",
    "\n",
    "\n",
    "`setup.ipynb` 5 May 2018\n",
    "===\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id='back-to-top'></a>\n",
    "\n",
    "\n",
    "### [*Preliminaries*](#preliminaries)\n",
    "\n",
    "### [*Creation of the DynamoDB databases*](#creation)\n",
    "- [Creation of the dynamodb table called `demographics`](#create_table)\n",
    "- [Creation of the metadata table called `demographics_meta`](#create_meta)  \n",
    "\n",
    "### [*Creation of S3 buckets*](#management)\n",
    "- [Bucket `pubmedcentral_oa`](#create_pubmedcentral_oa)\n",
    "- [Bucket `pubminer-upload`](#create_pubminer_upload)\n",
    "\n",
    "### [*(optional) Population of the DynamoDB and S3*](#populate)\n",
    "- [Get the list of PMCIDs and PMIDs](#getPMCIDs)   \n",
    "- [Batch populate the table with batch write](#batch_from_web)   \n",
    "- [Populate the table from JSON](#populate_from_json)  \n",
    "\n",
    "### [*Creation of Lambda function packages*](#lambda)\n",
    "- [GetPMCUpdatesFromCSV](#getupdatePMC)   \n",
    "- [DownloadPMC-OAFileToS3](#download)\n",
    "- [SentenceMinerOnEC2Instance](#sentence)\n",
    "- [TruncateTable](#truncate)\n",
    "- [UpdateStatsInDemographicsMeta](#stats)\n",
    "\n",
    "### [*Cloud Formation template*](#formation)\n",
    "- [Create the template](#CFtemplate)   \n",
    "- [Upload lamda packages to `pubminer_upload`](#uploadLambdas)\n",
    "- [Launch the Cloud Formation stack](#launch_stack)\n",
    "- [Tear down the Cloud Formation stack](#tear_down_stack)\n",
    "\n",
    "### [*Testing the installation*](#testing)\n",
    "- [Test events for AWS Lambda functions](#Testlambda)\n",
    "- [BeautifulSoup tests](#crummytest)   \n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backend services on AWS:  DynamoDB, S3 buckets, and AWS Lambda setup\n",
    "-----\n",
    "\n",
    "The backend elements are running on AWS using the services AWS Lambda, DynamoDB, IAM, S3, EC2, Cloud Formation and CloudWatch. The installation and configuration of the DynamoDB tables and the S3 buckets has been performed with the boto3 library in Python 3.6. The IAM roles and policies, the AWS Lambda functions and the associated event triggers are created with a AWS Cloud Formation template. The entire backend can be installed from scratch using Python and boto3, first creating the database tables and S3 buckets (or using the ones in place, as “pubmedcentral_oa” is about 40 GB) and then launching the Cloud Formation template with boto3. Every step is provided with instructions in a Jupyter notebook setup.ipynb.\n",
    "\n",
    "Currently all of the AWS services are running from a single account, and the AWS IAM service has been used to configure access for all team members, through a user Group and Role permissions. While the aspects of access management for our customer are out of scope for the project, a similar Group/Role approach can be taken after handover. The relevant sections of the Cloud Formation template would need adjusting for the Region and AccountID references.  As such, the code in this notebook will only work if run with the appropriate access to the account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[back-to-top](#back-to-top)\n",
    "<a id='preliminaries'></a>\n",
    "\n",
    "*Preliminaries - libraries to load*  \n",
    "=====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen \n",
    "from io import StringIO\n",
    "import csv\n",
    "import decimal\n",
    "\n",
    "\n",
    "import boto3\n",
    "from boto3 import resource\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "from datetime import datetime\n",
    "import bs4 as bs\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from lxml import html\n",
    "from unidecode import unidecode\n",
    "from lxml.etree import tostring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.cell { \n",
       "    margin-top:1em;\n",
       "    margin-bottom:1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render h1 {\n",
       "    font-size: 1.8em;\n",
       "    line-height:1.2em;\n",
       "    text-align:center;\n",
       "}\n",
       "\n",
       "div.text_cell_render h2 {\n",
       "margin-bottom: -0.2em;\n",
       "}\n",
       "\n",
       "table tbody tr td:first-child, \n",
       "table tbody tr th:first-child, \n",
       "table thead tr th:first-child, \n",
       "table tbody tr td:nth-child(4), \n",
       "table thead tr th:nth-child(4) {\n",
       "    background-color: #edf4e8;\n",
       "}\n",
       "\n",
       "div.text_cell_render { \n",
       "    font-family: 'Garamond';\n",
       "    font-size:1.4em;\n",
       "    line-height:1.3em;\n",
       "    padding-left:3em;\n",
       "    padding-right:3em;\n",
       "}\n",
       "\n",
       "div#notebook-container    { width: 95%; }\n",
       "div#menubar-container     { width: 65%; }\n",
       "div#maintoolbar-container { width: 99%; }\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Markdown CSS\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { \n",
    "    margin-top:1em;\n",
    "    margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 {\n",
    "    font-size: 1.8em;\n",
    "    line-height:1.2em;\n",
    "    text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 {\n",
    "margin-bottom: -0.2em;\n",
    "}\n",
    "\n",
    "table tbody tr td:first-child, \n",
    "table tbody tr th:first-child, \n",
    "table thead tr th:first-child, \n",
    "table tbody tr td:nth-child(4), \n",
    "table thead tr th:nth-child(4) {\n",
    "    background-color: #edf4e8;\n",
    "}\n",
    "\n",
    "div.text_cell_render { \n",
    "    font-family: 'Garamond';\n",
    "    font-size:1.4em;\n",
    "    line-height:1.3em;\n",
    "    padding-left:3em;\n",
    "    padding-right:3em;\n",
    "}\n",
    "\n",
    "div#notebook-container    { width: 95%; }\n",
    "div#menubar-container     { width: 65%; }\n",
    "div#maintoolbar-container { width: 99%; }\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='creation'></a>\n",
    "\n",
    "*Creation of the DynamoDB databases*  \n",
    "=====\n",
    "\n",
    "The DynamoDB tables “demographics” and “demographics_meta”  are created with just the primary key elements, and throughput capacities. The remaining attributes are created populated by the Lambda functions. The tables are created in setup.ipynb using boto3. There are also a full set of maintenance functions in the Jupyter notebook create_table.ipynb (batch populate, inquiring about the tables, database queries, updating, deleting items, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='create_table'></a>\n",
    "\n",
    "\n",
    "Creation the DynamoDB table called `demographics`\n",
    "----\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "This cell creates the main DynamoDB table that will hold the demographic inforamtion on the articles. The table will be created in the **us-east-1** AWS Region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table status: CREATING\n",
      "Item count: 0\n"
     ]
    }
   ],
   "source": [
    "dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')\n",
    "\n",
    "try:\n",
    "    table = dynamodb_client.create_table(\n",
    "        TableName='demographics',\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'pmcid', \n",
    "                'KeyType': 'HASH'\n",
    "            }\n",
    "        ], \n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'pmcid', \n",
    "                'AttributeType': 'S'\n",
    "            }\n",
    "        ], \n",
    "        ProvisionedThroughput={\n",
    "            'ReadCapacityUnits': 50, \n",
    "            'WriteCapacityUnits': 50\n",
    "        },\n",
    "        StreamSpecification={\n",
    "        'StreamEnabled': True,\n",
    "        'StreamViewType': 'NEW_AND_OLD_IMAGES'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dynamodb_client.get_waiter('table_exists').wait(TableName='demographics')\n",
    "    print(\"Table status:\",  table['TableDescription']['TableStatus'])\n",
    "    print(\"Item count:\", table['TableDescription']['ItemCount'])\n",
    "    \n",
    "except dynamodb_client.exceptions.ResourceInUseException:\n",
    "    print(\"Table in use error - do you really want to recreate the table?\")\n",
    "    pass\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "except ValueError:\n",
    "    print(\"Could not convert data to an integer.\")\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='create_meta'></a>\n",
    "\n",
    "Creation of the metadata table called `demographics_meta`\n",
    "----\n",
    "\n",
    "-----\n",
    "\n",
    "This cell creates the DynamoDB table that will hold the metadata concerning the database updates. The table will be created in the us-east-1 region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table status: CREATING\n",
      "Item count: 0\n"
     ]
    }
   ],
   "source": [
    "dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')\n",
    "\n",
    "try:\n",
    "    table = dynamodb_client.create_table(\n",
    "        TableName='demographics_meta',\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'source', \n",
    "                'KeyType': 'HASH'\n",
    "            }\n",
    "        ], \n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'source', \n",
    "                'AttributeType': 'S'\n",
    "            },\n",
    "        ], \n",
    "        ProvisionedThroughput={\n",
    "            'ReadCapacityUnits': 5, \n",
    "            'WriteCapacityUnits': 5\n",
    "        },\n",
    "        StreamSpecification={\n",
    "        'StreamEnabled': True,\n",
    "        'StreamViewType': 'NEW_AND_OLD_IMAGES'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dynamodb_client.get_waiter('table_exists').wait(TableName='demographics_meta')\n",
    "    print(\"Table status:\",  table['TableDescription']['TableStatus'])\n",
    "    print(\"Item count:\", table['TableDescription']['ItemCount'])\n",
    "    \n",
    "except dynamodb_client.exceptions.ResourceInUseException:\n",
    "    print(\"Table in use error - do you really want to recreate the table?\")\n",
    "    pass\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "except ValueError:\n",
    "    print(\"Could not convert data to an integer.\")\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='management'></a>\n",
    "\n",
    "*Creation of S3 buckets*  \n",
    "=====\n",
    "\n",
    "\n",
    "The S3 bucket “pubmedcentral_oa” holds all of the XML files for the Open Access articles which are the sources of information in the database (these are .nxml files). The S3 bucket \"pubminer_upload\" contains the configuration files, such as the Cloud Formation template, the Sentence Miner application jar file, and the source packages for the AWS Lambda functions. Both of these S3 buckets are created using boto3 in the cells below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_pubmedcentral_oa'></a>\n",
    "\n",
    "S3 Bucket `pubmedcentral`\n",
    "------\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Thu, 10 May 2018 18:54:23 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'UcUsj4SUWmU0Pve7CcpLVnE344u+wLy36j1Lz2656Bu18mKM8wkKN+U9kKCVMHFiEMP9F+W0Spw=',\n",
       "   'x-amz-request-id': '3030D16D51FBBA7A'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'UcUsj4SUWmU0Pve7CcpLVnE344u+wLy36j1Lz2656Bu18mKM8wkKN+U9kKCVMHFiEMP9F+W0Spw=',\n",
       "  'RequestId': '3030D16D51FBBA7A',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.create_bucket(Bucket='pubmedcentral_oa')\n",
    "s3_resource.Bucket('pubmedcentral_oa').Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_pubminer_upload'></a>\n",
    "\n",
    "S3 Bucket `pubminer-upload`\n",
    "------\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.create_bucket(Bucket='pubminer-upload')\n",
    "s3_resource.Bucket('pubminer-upload').Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the contents and the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Thu, 10 May 2018 19:17:24 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'HjfaPUYJkDly7ZneaAKB3a5IpYNd5sJSRhVST3UMU2WwqZYB7PgscabkX65APa94AZy1IH/2vgk=',\n",
       "   'x-amz-request-id': 'A0B20433B3BB39C7'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'HjfaPUYJkDly7ZneaAKB3a5IpYNd5sJSRhVST3UMU2WwqZYB7PgscabkX65APa94AZy1IH/2vgk=',\n",
       "  'RequestId': 'A0B20433B3BB39C7',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "for key in s3_resource.Bucket('pubmedcentral_oa').objects.all():\n",
    "    key.delete()\n",
    "s3_resource.Bucket('pubmedcentral_oa').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "for key in s3_resource.Bucket('pubminer-upload').objects.all():\n",
    "    key.delete()\n",
    "s3_resource.Bucket('pubminer-upload').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='management'></a>\n",
    "\n",
    "*(optional) Population of the DynamoDB tables, and the necessary S3 files*  \n",
    "=====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The followinig two sections are optional**, to initially populate the `demographics` DynamoDB table. These steps are only necessary if not using the existing database.    \n",
    "\n",
    "\n",
    "If the entire backend needs to be installed from scratch, the demographics table can be populated, if necessary, for the entire set of PMCIDs, using the Python functions in the cell below (since an AWS Lambda function GetPMCUpdatesFromCSV of such magnitude would time out). Once the primary key items for “pmcid” are populated, the rest of the backend will automatically generate from the sequence of Lambda functions, triggered initially from the stream for the demographics DynamoDB table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='populate'></a>\n",
    "\n",
    "Get the list of articles to populate the database table\n",
    "----------\n",
    "\n",
    "-----\n",
    "\n",
    "Prior to populating the database, the full set of PMCIDs and PMIDs needs to be created, in the following cell (derived from the GetPMCUpdatesFromCSV Lambda function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a JSON data for the PMCIDs and PMIDs\n",
    "NUM_DAYS = 2\n",
    "def grabfrompubmed(num_days_delay):\n",
    "\n",
    "    ## Query pubmed with e-search  - returns JSON\n",
    "    ##\n",
    "    # build the query string\n",
    "    query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils//esearch.fcgi?db=pubmed&retmax=4659616&retmode=json\"\n",
    "    query += \"&datetype=edat&reldate=\" + str(num_days_delay)\n",
    "    query += \"&term=((randomized+controlled+trial%5Bpt%5D)+OR+(controlled+clinical+trial%5Bpt%5D)+OR+(randomized%5Btiab%5D+OR+randomised%5Btiab%5D)+OR+(placebo%5Btiab%5D)+OR+(drug+therapy%5Bsh%5D)+OR+(randomly%5Btiab%5D)+OR+(trial%5Btiab%5D)+OR+(groups%5Btiab%5D))+NOT+(animals%5Bmh%5D+NOT+humans%5Bmh%5D)\"\n",
    "\n",
    "    # grab the content from pubmed with e-search\n",
    "    print(\"Getting the OA CSV from NCMB FTP site ...\")\n",
    "    r = urlopen(query) \n",
    "    resp = json.loads(r.read().decode(r.info().get_param('charset') or 'utf-8'))\n",
    "    \n",
    "    # extract the list of target pubmed ids (pmids)\n",
    "    target_pmids = resp['esearchresult']['idlist']\n",
    "    print(\"\\nSample of the first 10 pmids returned by search:\")\n",
    "    print(target_pmids[:10])\n",
    "    print(\"Search of PubMed returned {} results\".format(len(target_pmids)))\n",
    "    \n",
    "    ## Grab the OA file list from the NCBI FTP site\n",
    "    ##\n",
    "    print(\"\\nGetting the OA CSV from NCMB FTP site ...\")\n",
    "    url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv'\n",
    "    data = urlopen(url).read().decode('ascii', 'ignore')\n",
    "    dataFile = StringIO(data)\n",
    "    csvReader = csv.reader(dataFile)\n",
    "    \n",
    "    targs = set(target_pmids)\n",
    "    merged_pmids = []\n",
    "    merged_pmcids = []\n",
    "    \n",
    "    # scan through the pairs of pubmed ids (pmids) and pubmed pmc oa ids (pmcids)  \n",
    "    # from the csv file and keep only those in the target list\n",
    "    for row in csvReader:\n",
    "        pmid = row[4]\n",
    "        if pmid in targs:\n",
    "            merged_pmids.append(pmid)\n",
    "            merged_pmcids.append(row[2][3:])\n",
    "    \n",
    "    # get the number of articles to update\n",
    "    num_updates = len(merged_pmcids)\n",
    "    print(\"\\nNumber of PMC OpenAccess updates: {}\".format(num_updates))\n",
    "    \n",
    "    # some variables to hold todays date - both long and short format        \n",
    "    now = str(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    nowlong = str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ## Create the JSON objects - one for a JSON file and the other for immediately populatin \n",
    "    ##\n",
    "    ##############################################\n",
    "    \n",
    "    # create a JSON object with the status elements for the demographics_meta table\n",
    "    # it will also be saved to the S3 bucket, for good measure\n",
    "    jsondata = {\n",
    "        \"date_updated\": nowlong, \n",
    "        \"pmcids\": merged_pmcids,\n",
    "        \"pmids\": merged_pmids,\n",
    "        \"items\": str(num_updates)\n",
    "    }\n",
    "\n",
    "    # create an list that contains the items to update in the demographics table\n",
    "    jsonitem = []\n",
    "    for i in range(num_updates):\n",
    "        jsonitem.append(\n",
    "            {\n",
    "            \"pmcid\": merged_pmcids[i],\n",
    "            \"pmid\":  merged_pmids[i],\n",
    "            \"date_processed\":  now\n",
    "            }) \n",
    "    return jsondata, jsonitem\n",
    "    \n",
    "def puts3item(jsondata, filename):\n",
    "    # function to put the JSON file of newRCTs.json to the S3 bucket for sentence miner\n",
    "    # and also to put the full JSON file of metadata to the S3 bucket for safekeeping\n",
    "    s3bucketfile = filename\n",
    "    s3bucket = \"pubminer-upload\"\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    obj = s3_resource.Object(s3bucket,s3bucketfile)\n",
    "    obj.put(Body=json.dumps(jsondata), ACL='public-read')\n",
    "    return \"Success\"\n",
    "\n",
    "def putdbitem(jsonitem):\n",
    "    # function to put the items from the list to the dynamodb table \n",
    "    dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "    \n",
    "    table = dynamodb_resource.Table('demographics')\n",
    "    for i in range(len(jsonitem)):\n",
    "        table.put_item(\n",
    "            Item=jsonitem[i]\n",
    "        )\n",
    "    return \"Success\"\n",
    "\n",
    "def putdbmetaitem(jsondata):\n",
    "    # function to update the attributes of the demographics item in the dynamodb meta table \n",
    "    dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "    table = dynamodb_resource.Table('demographics')\n",
    "    total_count = table.item_count\n",
    "    table = dynamodb_resource.Table('demographics_meta')\n",
    "    \n",
    "    table.update_item(\n",
    "        Key={'source': 'demographics'},\n",
    "        UpdateExpression=\"set date_updated=:u, items_updated=:i, items_downloaded=:d, items_total=:m, with_tables=:t, with_sentences=:s, pmcids=:c, pmids=:p\", \n",
    "        ExpressionAttributeValues={\n",
    "            ':u': str(jsondata['date_updated']),\n",
    "            ':i': decimal.Decimal(jsondata['items']),\n",
    "            ':d': decimal.Decimal(0),\n",
    "            ':m': total_count,\n",
    "            ':t': decimal.Decimal(0),\n",
    "            ':s': decimal.Decimal(0),       \n",
    "            ':c': jsondata['pmcids'],\n",
    "            ':p': jsondata['pmids'],            \n",
    "            },\n",
    "        ReturnValues=\"UPDATED_NEW\"\n",
    "    )\n",
    "    return \"Success\"\n",
    "\n",
    "try:\n",
    "    jsondata, jsonitem = grabfrompubmed(NUM_DAYS)\n",
    "    num_updates = len(jsondata['pmcids'])\n",
    "    if num_updates > 0:\n",
    "        if not putdbitem(jsonitem):\n",
    "            raise Exception('Writing updates to demographics DB failed')\n",
    "        if not putdbmetaitem(jsondata):\n",
    "            raise Exception('Writing updates to demographics_meta failed')\n",
    "        if not puts3item(jsondata['pmcids'], \"newRCTs.json\"):\n",
    "            raise Exception('Writing newRCTs.json to S3 failed')\n",
    "        # put the full meta item in the S3 bucket\n",
    "        if not puts3item(jsondata, \"new_item.updates\"):\n",
    "            raise Exception('Writing new_item.updates to S3 failed')\n",
    "\n",
    "except:\n",
    "    print('\\nGrab updates function failed!')\n",
    "    raise\n",
    "else:\n",
    "    print('\\nGrab updates function passed!')\n",
    "    print(str(datetime.now()))\n",
    "finally:\n",
    "    print('Grab updates function complete at {}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the JSON file to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            if o % 1 > 0:\n",
    "                return float(o)\n",
    "            else:\n",
    "                return int(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "    \n",
    "data_to_dump = json.dumps(jsonitem, cls=DecimalEncoder)\n",
    "f = open(\"output_dict.json\",\"w\")\n",
    "f.write(data_to_dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='batch_from_web'></a>\n",
    "\n",
    "\n",
    "Batch populate the table with batch write\n",
    "------\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch put from the JSON object `jsonitem`\n",
    "\n",
    "#### *This is what has populated the current table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb_resource.Table('demographics')\n",
    "\n",
    "with table.batch_writer() as batch:\n",
    "    for i in range(len(demographic_json)):\n",
    "        batch.put_item(\n",
    "            Item=jsonitem[i]\n",
    "        )\n",
    "        if (i % 1000 == 0):\n",
    "            print (\"The item_count is: %s \" % (i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='populate_from_json'></a>\n",
    "\n",
    "\n",
    "Populate the table from JSON\n",
    "------\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal  put from the JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb_resource.Table('demographics')\n",
    "\n",
    "for i in range(len(demographic_json)):\n",
    "    table.put_item(\n",
    "        Item=jsonitem[i]\n",
    "    )\n",
    "#    if (i % 10 == 0):\n",
    "#        print (\"The item_count is: %s \" % (i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal  put from a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "input_file = \"output_dict.json\"\n",
    "\n",
    "try:\n",
    "    table = dynamodb_resource.Table('demographics')\n",
    "    print(\"Instantiate a table: \",table.creation_date_time)\n",
    "    print(\"Ready to load data\\n\")\n",
    "    incr = 0\n",
    "    with open(input_file) as json_file:\n",
    "        itemset = json.load(json_file, parse_float = decimal.Decimal)\n",
    "        for item in itemset:\n",
    "            incr += 1\n",
    "            table.put_item(\n",
    "               Item={\n",
    "                   'pmcid': item,\n",
    "                   'pmcid': pmcid,\n",
    "                   'date_processed': str(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "                }\n",
    "            )\n",
    "            \n",
    "except dynamodb_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Table does not exist\")\n",
    "    pass            \n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "except ValueError:\n",
    "    print(\"Could not convert data to an integer.\")\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='lambda'></a>\n",
    "\n",
    "*Creation of Lambda function packages*  \n",
    "=====\n",
    "\n",
    "The Lambda functions can be setup using the an AWS Cloud Formation template. The template, as well as the Lambda function packages are created using Python 3.6 in the cells below. These cells are used to create (and adjust if necessary) the .py files for the  Lambda functions and can then be used to upload them to S3. Similarly, the Cloud Formation template is created and uploaded to S3 from the notebook, and then the Cloud Formation stack is launched using boto3.   \n",
    "\n",
    "Each Lambda function is described in a dedicated section, along with the trigger events, and the example ARN for the running platform. A brief overview of the Lambda functions follows:   \n",
    "\n",
    "- “GetPMCUpdatesFromCSV” does the search, intersection of PMIDs and PMCIDs for the Open Access subset, and the initial population of items in the demographics and demographics_meta tables.    \n",
    "- “DownloadPMC-OAFileToS3” does the download of files from PMC-OA and the upload to S3.    \n",
    "- “TruncateTable” does the table truncation. The AWS Lambda package includes the *BeautifulSoup4* and *lxml* packages, which are not included by default on Lambda.    \n",
    "- The sentence extraction is performed on an EC2 instance, which is launched with a “user-data” script to perform the installation of the packages and necessary .jar file to run. The file “SentenceMinerOnEC2Instance” does these steps.    \n",
    "- The function \"UpdateStatsInDemographicsMeta\" does a final update to the demographics_meta dynamodb table with the statistics of the latest update to the demographics table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='getupdatePMC'></a>\n",
    "\n",
    "GetPMCUpdatesFromCSV\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Does the search, intersection of PMIDs and PMCIDs for the Open Access subset, and the initial population of items in the demographics and demographics_meta tables.   \n",
    "\n",
    "### Associated Role & Policy\n",
    "`PM_lambda_dynamo_S3_role`\n",
    "\n",
    "\n",
    "### trigger:     \n",
    "`ScheduleS3AndDynamodbUpdateEvent`  \n",
    "`Schedule expression: cron(00 6 * * ? *)Description: ScheduleS3AndDynamodbUpdateEvent`  \n",
    "\n",
    "### test event:\n",
    "`myTestEvent`  (Vanilla trigger)\n",
    "{\n",
    "  \"key3\": \"value3\",\n",
    "  \"key2\": \"value2\",\n",
    "  \"key1\": \"value1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lambda_update_project/lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lambda_update_project/lambda_function.py\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen \n",
    "from io import StringIO\n",
    "import csv\n",
    "import boto3\n",
    "import decimal\n",
    "\n",
    "NUM_DAYS = os.environ['number_days'] \n",
    "\n",
    "def grabfrompubmed(num_days_delay):\n",
    "    ##############################################\n",
    "    ##\n",
    "    ## Query pubmed with e-search  - returns JSON\n",
    "    ##\n",
    "    ##############################################\n",
    "    \n",
    "    # build the query string\n",
    "    query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils//esearch.fcgi?db=pubmed&retmax=4659616&retmode=json\"\n",
    "    query += \"&datetype=edat&reldate=\" + str(num_days_delay)\n",
    "    query += \"&term=((randomized+controlled+trial%5Bpt%5D)+OR+(controlled+clinical+trial%5Bpt%5D)+OR+(randomized%5Btiab%5D+OR+randomised%5Btiab%5D)+OR+(placebo%5Btiab%5D)+OR+(drug+therapy%5Bsh%5D)+OR+(randomly%5Btiab%5D)+OR+(trial%5Btiab%5D)+OR+(groups%5Btiab%5D))+NOT+(animals%5Bmh%5D+NOT+humans%5Bmh%5D)\"\n",
    "\n",
    "    # grab the content from pubmed with e-search\n",
    "    print(\"Getting the OA CSV from NCMB FTP site ...\")\n",
    "    r = urlopen(query) \n",
    "    resp = json.loads(r.read().decode(r.info().get_param('charset') or 'utf-8'))\n",
    "    \n",
    "    # extract the list of target pubmed ids (pmids)\n",
    "    target_pmids = resp['esearchresult']['idlist']\n",
    "    print(\"\\nSample of the first 10 pmids returned by search:\")\n",
    "    print(target_pmids[:10])\n",
    "    print(\"Search of PubMed returned {} results\".format(len(target_pmids)))\n",
    "    \n",
    "    ##############################################\n",
    "    ##\n",
    "    ## Grab the OA file list from the NCBI FTP site\n",
    "    ##\n",
    "    ##############################################\n",
    "    print(\"\\nGetting the OA CSV from NCMB FTP site ...\")\n",
    "    url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv'\n",
    "    data = urlopen(url).read().decode('ascii', 'ignore')\n",
    "    dataFile = StringIO(data)\n",
    "    csvReader = csv.reader(dataFile)\n",
    "    \n",
    "    targs = set(target_pmids)\n",
    "    merged_pmids = []\n",
    "    merged_pmcids = []\n",
    "    \n",
    "    # scan through the pairs of pubmed ids (pmids) and pubmed pmc oa ids (pmcids)  \n",
    "    # from the csv file and keep only those in the target list\n",
    "    for row in csvReader:\n",
    "        pmid = row[4]\n",
    "        if pmid in targs:\n",
    "            merged_pmids.append(pmid)\n",
    "            merged_pmcids.append(row[2][3:])\n",
    "    \n",
    "    # get the number of articles to update\n",
    "    num_updates = len(merged_pmcids)\n",
    "    print(\"\\nNumber of PMC OpenAccess updates: {}\".format(num_updates))\n",
    "    \n",
    "    # some variables to hold todays date - both long and short format        \n",
    "    now = str(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    nowlong = str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    \n",
    "    ##############################################\n",
    "    ##\n",
    "    ## Create the objects for the DB tables and S3\n",
    "    ##\n",
    "    ##############################################\n",
    "    \n",
    "    # create a JSON object with the status elements for the demographics_meta table\n",
    "    # it will also be saved to the S3 bucket, for good measure\n",
    "    jsondata = {\n",
    "        \"date_updated\": nowlong, \n",
    "        \"pmcids\": merged_pmcids,\n",
    "        \"pmids\": merged_pmids,\n",
    "        \"items\": str(num_updates)\n",
    "    }\n",
    "\n",
    "    # create an list that contains the items to update in the demographics table\n",
    "    jsonitem = []\n",
    "    for i in range(num_updates):\n",
    "        jsonitem.append(\n",
    "            {\n",
    "            \"pmcid\": merged_pmcids[i],\n",
    "            \"pmid\":  merged_pmids[i],\n",
    "            \"date_processed\":  now\n",
    "            }) \n",
    "    return jsondata, jsonitem\n",
    "    \n",
    "def puts3item(jsondata, filename):\n",
    "    # function to put the JSON file of newRCTs.json to the S3 bucket for sentence miner\n",
    "    # and also to put the full JSON file of metadata to the S3 bucket for safekeeping\n",
    "    s3bucketfile = filename\n",
    "    s3bucket = \"pubminer-upload\"\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    obj = s3_resource.Object(s3bucket,s3bucketfile)\n",
    "    obj.put(Body=json.dumps(jsondata), ACL='public-read')\n",
    "    return \"Success\"\n",
    "\n",
    "def putdbitem(jsonitem):\n",
    "    # function to put the items from the list to the dynamodb table \n",
    "    dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "    \n",
    "    table = dynamodb_resource.Table('demographics')\n",
    "    for i in range(len(jsonitem)):\n",
    "        table.put_item(\n",
    "            Item=jsonitem[i]\n",
    "        )\n",
    "    return \"Success\"\n",
    "\n",
    "def putdbmetaitem(jsondata):\n",
    "    # function to update the attributes of the demographics item in the dynamodb meta table \n",
    "    dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "    table = dynamodb_resource.Table('demographics')\n",
    "    total_count = table.item_count\n",
    "    table = dynamodb_resource.Table('demographics_meta')\n",
    "    \n",
    "    table.update_item(\n",
    "        Key={'source': 'demographics'},\n",
    "        UpdateExpression=\"set date_updated=:u, items_updated=:i, items_downloaded=:d, items_total=:m, with_tables=:t, with_sentences=:s, pmcids=:c, pmids=:p\", \n",
    "        ExpressionAttributeValues={\n",
    "            ':u': str(jsondata['date_updated']),\n",
    "            ':i': decimal.Decimal(jsondata['items']),\n",
    "            ':d': decimal.Decimal(0),\n",
    "            ':m': total_count,\n",
    "            ':t': decimal.Decimal(0),\n",
    "            ':s': decimal.Decimal(0),       \n",
    "            ':c': jsondata['pmcids'],\n",
    "            ':p': jsondata['pmids'],            \n",
    "            },\n",
    "        ReturnValues=\"UPDATED_NEW\"\n",
    "    )\n",
    "    return \"Success\"\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print('\\nChecking for new articles at PubMed...')\n",
    "\n",
    "    try:\n",
    "        jsondata, jsonitem = grabfrompubmed(NUM_DAYS)\n",
    "        num_updates = len(jsondata['pmcids'])\n",
    "        if num_updates > 0:\n",
    "            if not putdbitem(jsonitem):\n",
    "                raise Exception('Writing updates to demographics DB failed')\n",
    "            if not putdbmetaitem(jsondata):\n",
    "                raise Exception('Writing updates to demographics_meta failed')\n",
    "            if not puts3item(jsondata['pmcids'], \"newRCTs.json\"):\n",
    "                raise Exception('Writing newRCTs.json to S3 failed')\n",
    "            # put the full meta item in the S3 bucket\n",
    "            if not puts3item(jsondata, \"new_item.updates\"):\n",
    "                raise Exception('Writing new_item.updates to S3 failed')\n",
    "                \n",
    "    except:\n",
    "        print('\\nGrab updates function failed!')\n",
    "        raise\n",
    "    else:\n",
    "        print('\\nGrab updates function passed!')\n",
    "        return str(datetime.now())\n",
    "    finally:\n",
    "        print('Grab updates function complete at {}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='download'></a>\n",
    "\n",
    "DownloadPMC-OAFileToS3\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Does the download of files from PMC-OA and the upload to S3.     \n",
    "\n",
    "### Associated Role & Policy\n",
    "`PM_lambda_dynamodb_execution_role`\n",
    "\n",
    "\n",
    "### trigger:     \n",
    "`DynamodbUpdateEvent`  \n",
    "`demographics`  table stream for updates\n",
    "\n",
    "\n",
    "### test event:\n",
    "`UpdateToDynamoDB`  (see testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lambda_download_project/lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lambda_download_project/lambda_function.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "import boto3\n",
    "import glob\n",
    "import tarfile\n",
    "import json\n",
    "import decimal\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "save_bucket_name = 'pubmedcentral_oa'\n",
    "dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb_resource.Table('demographics')\n",
    "status_table = dynamodb_resource.Table('demographics_meta')\n",
    "\n",
    "#s3bucketfile = \"new_updates.json\"\n",
    "\n",
    "# test tar.gz to see if the unzipping from the website works\n",
    "#thetarfile = \"ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_package/ca/f2/PMC5855623.tar.gz\"\n",
    "\n",
    "# the url for getting the file references\n",
    "url = 'http://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id=PMC'\n",
    "\n",
    "# test list of pmcids to test the download function outside the dynamodb stream\n",
    "#pmcidlist = ['3778263', '4473156', '5771285', '4264693', '4210730', '4578256', '5424460', '5541470', '4634690', '4548150']\n",
    "#pmcidlist = ['5838818', '5869227', '5869228', '2842548']\n",
    "\n",
    "def do_the_download(pmcidlist):\n",
    "    print(\"Downloading the articles from the website...\")\n",
    "    for pmcid in pmcidlist:\n",
    "        try:\n",
    "            if not download_file_from_ftp(url, pmcid):\n",
    "                raise Exception('Download from FTP failed - decrementing counter and deleting item')\n",
    "            else:\n",
    "                if not upload_file_to_s3(pmcid):\n",
    "                    raise Exception('Upload to S3 failed')\n",
    "                else:\n",
    "                    # increment the items_downloaded counter on the metadata table\n",
    "                    if not increment_counter(status_table):\n",
    "                        raise Exception('Incrementing metadata table failed')\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "            # remove item and decrement the items_updated counter on the metadata table\n",
    "            if not decrement_counter(status_table, pmcid):\n",
    "                print('decrementing metadata table failed')\n",
    "            if not remove_from_database(table, pmcid):\n",
    "                print('Removal from database table failed')\n",
    "    return \"Success\"\n",
    "                \n",
    "def download_file_from_ftp(url, pmcid):\n",
    "    # assemble the query url\n",
    "    query = url + pmcid\n",
    "    # grab the page with urlopen\n",
    "    page = urlopen(query).read()\n",
    "    #print(page)\n",
    "    # get the xml tree from the returned page\n",
    "    tree = ET.fromstring(page)\n",
    "    #print(tree)\n",
    "    # find the link element for the tar.gz file\n",
    "    link = tree.find(\".//link[@format='tgz']\")\n",
    "    if link is not None:\n",
    "        thetarfile = link.get('href')\n",
    "        print(thetarfile)\n",
    "        # use urlopen again to get the file from the FTP site as a ftpstream\n",
    "        ftpstream = urlopen(thetarfile)\n",
    "        thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
    "        # get the .nxml file out of the zipped file, and save to the /tmp folder\n",
    "        for tarinfo in thetarfile:\n",
    "            if (tarinfo.name[-4:] == 'nxml'):\n",
    "                thetarfile.extract(tarinfo, path='/tmp/')\n",
    "        thetarfile.close()\n",
    "        print(\"FTP download success\")\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        print(\"FTP download failed\")\n",
    "                \n",
    "def upload_file_to_s3(pmcid):\n",
    "    # get the full path and name of the file from the /tmp folder\n",
    "    filename_in = str(glob.glob('/tmp/PMC'+str(pmcid)+'/*.nxml')[0])\n",
    "    print(filename_in)\n",
    "    # set the format of the filename for the S3 bucket\n",
    "    filename_out = 'PMC'+str(pmcid)+ '.nxml'\n",
    "    # upload the file to S3\n",
    "    try:\n",
    "        s3_client.upload_file(filename_in, save_bucket_name, filename_out, ExtraArgs={'ACL': 'public-read'})\n",
    "        print('S3 upload passed!')\n",
    "        return \"Success\"\n",
    "    except:\n",
    "        print('S3 upload failed!')\n",
    "        \n",
    "def puts3item(jsondata, filename):\n",
    "    # function to put the JSON file of newRCTs.json to the S3 bucket for sentence miner\n",
    "    # and also to put the full JSON file of metadata to the S3 bucket for safekeeping\n",
    "    s3bucketfile = filename\n",
    "    s3bucket = \"pubminer-upload\"\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    obj = s3_resource.Object(s3bucket,s3bucketfile)\n",
    "    obj.put(Body=json.dumps(jsondata), ACL='public-read')\n",
    "    return \"Success\"\n",
    "\n",
    "def increment_counter(table):\n",
    "    # increment the counter on items_downloaded in the demographics_meta table\n",
    "    response = table.update_item(\n",
    "        Key={\n",
    "            'source': 'demographics'\n",
    "        },\n",
    "        UpdateExpression=\"set items_downloaded = items_downloaded + :val\",\n",
    "        ExpressionAttributeValues={\n",
    "            ':val': decimal.Decimal(1)\n",
    "        },\n",
    "        ReturnValues=\"UPDATED_NEW\"\n",
    "    )\n",
    "    print(\"Incremented download counter\")\n",
    "    return \"Success\"\n",
    "    \n",
    "def decrement_counter(table, pmcid):\n",
    "    # function to make modification to the demographics_meta table\n",
    "    # first get the current demographics item and see if there are updates\n",
    "    print(\"Decrementing counter...\")\n",
    "    response = table.get_item(\n",
    "                    Key={\n",
    "                        'source': 'demographics'\n",
    "                    }\n",
    "                )\n",
    "    current_status = response['Item']\n",
    "    #print(\"response\", current_status['pmcids'])\n",
    "    if (current_status['items_updated'] > 0 ):\n",
    "\n",
    "        # first get the list of PMCIDs and PMIDs\n",
    "        pmcids = current_status['pmcids']\n",
    "        pmids = current_status['pmids']\n",
    "        try:\n",
    "            # make modification to the demographics_meta table\n",
    "            popind =  pmcids.index(pmcid)\n",
    "            # take out the PMCID\n",
    "            pmcids.remove(pmcid)\n",
    "            # take out the PMID\n",
    "            pmids.pop(popind)\n",
    "            # decrement the counter on items_uploaded \n",
    "            response = table.update_item(\n",
    "                Key={\n",
    "                    'source': 'demographics'\n",
    "                },\n",
    "                UpdateExpression=\"set items_updated=items_updated-:val, pmcids=:l, pmids=:m\",\n",
    "                ExpressionAttributeValues={\n",
    "                    ':val': decimal.Decimal(1),\n",
    "                    ':l': pmcids,\n",
    "                    ':m': pmids\n",
    "                },\n",
    "                ReturnValues=\"UPDATED_NEW\"\n",
    "            )\n",
    "            jsondata = {\n",
    "                \"time_processed\": current_status['time_processed'], \n",
    "                \"pmcids\": pmcids,\n",
    "                \"pmids\": pmids,\n",
    "                \"items\": str(len(pmcids))\n",
    "            }\n",
    "            if not puts3item(jsondata, \"new_item.updates\"):\n",
    "                raise Exception('Writing new_item.updates to S3 failed')\n",
    "            if not puts3item(jsondata['pmcids'], \"newRCTs.json\"):\n",
    "                raise Exception('Writing newRCTs.json to S3 failed')\n",
    "            print(\"Decremented upload counter\")\n",
    "        except ValueError:\n",
    "            print(\"Decrement upload counter not necessary\")\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "    else: \n",
    "        print(\"No changes to demographics_meta table necessary\")\n",
    "    return \"Success\"\n",
    "\n",
    "def remove_from_database(table, pmcid):\n",
    "    print(\"Removing from database...\")\n",
    "    try:\n",
    "        response = table.get_item(Key={'pmcid': pmcid})[\"Item\"]\n",
    "        print(response)\n",
    "    except KeyError:\n",
    "        print(\"PMCID not in database - OK\")\n",
    "    except:\n",
    "        print(\"Error deleting PMCID from database\")\n",
    "    else: \n",
    "        print(\"Deleting unavailable PMCID from database\")\n",
    "        response = table.delete_item(Key={'pmcid': pmcid})\n",
    "        #print(response)\n",
    "    finally:\n",
    "        print(\"Delete PMCID from DB finished\")\n",
    "        return \"Success\"\n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    print('Receiving {} records ...'.format(len(event['Records'])))\n",
    "    pmcidlist = []\n",
    "    for record in event['Records']:\n",
    "        print(record)\n",
    "        # add to the list if the record is an INSERT\n",
    "        if (record['eventName'] == 'INSERT') :\n",
    "            print(\"Update from Insert\")\n",
    "            print(\"PMC\"+record['dynamodb']['Keys']['pmcid']['S'])\n",
    "            pmcidlist.append(record['dynamodb']['Keys']['pmcid']['S'])\n",
    "        # add to the list if the record is an MODIFY which lost information   \n",
    "        elif (record['eventName'] == 'MODIFY') :\n",
    "            if (len(record['dynamodb']['NewImage']) < len(record['dynamodb']['OldImage'])):\n",
    "                print(\"Update from Modify\")\n",
    "                print(\"PMC\"+record['dynamodb']['Keys']['pmcid']['S'])\n",
    "                pmcidlist.append(record['dynamodb']['Keys']['pmcid']['S'])\n",
    "        else:\n",
    "            print('No update of table')\n",
    "    # download the articles from the list of pmcids\n",
    "    try:\n",
    "        if (len(pmcidlist) > 0):\n",
    "            if not do_the_download(pmcidlist):\n",
    "                raise Exception('Download failed')\n",
    "        else: \n",
    "            return\n",
    "    except:\n",
    "        print('\\nDownloading of article to S3 failed!')\n",
    "        raise\n",
    "    else:\n",
    "        print('\\nDownloadPMC-OAFileToS3 passed!')\n",
    "    finally:\n",
    "        print('\\nDownloadPMC-OAFileToS3 complete at {}'.format(str(datetime.now())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='sentence'></a>\n",
    "\n",
    "SentenceMinerOnEC2Instance\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "The sentence extraction is performed on an EC2 instance, which is launched with a “user-data” script to perform the installation of the packages and necessary .jar file to run. The file “SentenceMinerOnEC2Instance” does these steps. \n",
    "\n",
    "### Associated Role & Policy\n",
    "`PM_lambda_start_stop_ec2`\n",
    "\n",
    "\n",
    "### trigger:     \n",
    "`DynamodbUpdateEvent`  \n",
    "`demographics_meta`  table stream for updates\n",
    "\n",
    "\n",
    "### test event:\n",
    "`DynamoTriggerExtraction`  (see testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Replace Account ID in XXXXXXXXXXXX below** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lambda_sentences_project/lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lambda_sentences_project/lambda_function.py\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "REGION = 'us-east-1' \n",
    "AMI = 'ami-467ca739'\n",
    "INSTANCE_TYPE = 't2.xlarge'\n",
    "instance_profile_arn = 'arn:aws:iam::XXXXXXXXXXXX:instance-profile/PubMinerInstanceWithSSM'\n",
    "\n",
    "ec2 = boto3.resource('ec2', region_name=REGION)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    print('Receiving {} records ...'.format(len(event['Records'])))\n",
    "\n",
    "    for record in event['Records']:\n",
    "        print(record)\n",
    "        if (record['eventName'] == 'MODIFY') :\n",
    "            if (record['dynamodb']['NewImage']['items_downloaded']['N'] > record['dynamodb']['OldImage']['items_downloaded']['N']):\n",
    "                if (record['dynamodb']['NewImage']['items_downloaded']['N'] == record['dynamodb']['NewImage']['items_updated']['N']):                \n",
    "                    print(\"update from Modify: items_downloaded =  items_updated\")\n",
    "                    print((record['dynamodb']['NewImage']['items_downloaded']['N']))\n",
    "                    print('Extracting the sentences...')\n",
    "                    try:\n",
    "                        user_data_string = \"\"\"#!/bin/bash\n",
    "yum update -y\n",
    "yum install java-1.8.0 -y\n",
    "yum remove java-1.7.0-openjdk -y\n",
    "cd /home/ec2-user/\n",
    "pip install boto3\n",
    "wget http://pubminer-upload.s3.amazonaws.com/sentences_status.py\n",
    "wget http://pubminer-upload.s3.amazonaws.com/newRCTs.json\n",
    "wget http://pubminer-upload.s3.amazonaws.com/PubMiner_s3.jar\n",
    "wait\n",
    "java -cp PubMiner_s3.jar ReadXMLFile\n",
    "wait\n",
    "python sentences_status.py\n",
    "shutdown -H +1\n",
    "\"\"\"\n",
    "\n",
    "                        print('Running script:')\n",
    "                        print(user_data_string)\n",
    "                        \n",
    "                        subnet = ec2.Subnet('subnet-a5899b8a')\n",
    "                        instance = subnet.create_instances(\n",
    "                            ImageId=AMI,\n",
    "                            InstanceType=INSTANCE_TYPE,\n",
    "                            MinCount=1,\n",
    "                            MaxCount=1,\n",
    "                            KeyName = 'pubminerinstance',\n",
    "                            IamInstanceProfile = {\n",
    "                                'Arn': instance_profile_arn\n",
    "                            },\n",
    "                            SecurityGroupIds = [\n",
    "                            'sg-d023d698',\n",
    "                            ],\n",
    "                            InstanceInitiatedShutdownBehavior='terminate', \n",
    "                            UserData=user_data_string \n",
    "                        )\n",
    "                        \n",
    "                        print(\"New instance created.\")\n",
    "                        print(instance)\n",
    "                    except:\n",
    "                        print('Extract sentences failed!')\n",
    "                        raise\n",
    "                    else:\n",
    "                        print('Extract sentences passed!')\n",
    "                        return str(datetime.now())\n",
    "                    finally:\n",
    "                        print('Extract sentences is launching at {}'.format(str(datetime.now())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='truncate'></a>\n",
    "\n",
    "TruncateTable\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Does the table truncation. The AWS Lambda package includes the BeautifulSoup4 and lxml packages, which are not included by default on Lambda.   \n",
    "\n",
    "### Associated Role & Policy\n",
    "`PM_lambda_dynamo_S3_role`\n",
    "\n",
    "\n",
    "### trigger:     \n",
    "pubmedcentral_oa\n",
    "arn:aws:s3:::pubmedcentral_oa\n",
    "Suffix: .nxmlEvent type: ObjectCreatedNotification name: TriggerTruncateTableEventOnCreate\n",
    "\n",
    "### test event:\n",
    "`TestPutToBucket`  (see testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lambda_truncate_project/lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lambda_truncate_project/lambda_function.py\n",
    "import re\n",
    "import boto3\n",
    "import glob\n",
    "import bs4 as bs\n",
    "from datetime import datetime\n",
    "import decimal\n",
    "\n",
    "regexTable1inTable = '(tab(:?.|le|le.|le-|leau|ela)? *?(1[.]?|I[.]?)$)'\n",
    "regexInTable = re.compile(regexTable1inTable, re.I)\n",
    "\n",
    "regexAge = '\\\\b(age[d|s]?)\\\\b'\n",
    "regexTime1 = '(\\\\S+\\\\s*\\\\d+(.\\\\d+)?)$'\n",
    "regexTime2 = '\\\\b(year[s]?|week[s]?|month[s]?|mean|(max|min)(imum)?)\\\\b'\n",
    "regexTime = re.compile('|'.join([regexTime1, regexTime2]), re.I)\n",
    "\n",
    "regexSex = '\\\\b(sex(es)?|gender[s]?|male[s]?|female[s]?|(wo)?men[s]?|boy[s]?|girl[s]?)\\\\b'\n",
    "regexRace1 = '\\\\b(race[s]?|ethnic(?:ity|ities)?|asian[s]?|white[s]?|black[s]?|aboriginal[s]?|native[s]?|african\\samerican[s]?|american\\sindian[s]]?)\\\\b'\n",
    "regexRace2 = '\\\\b(hispanic[s]?|caucasian[s]?|latin[a|o][s]?|chican[a|o][s]?|spanish|puerto\\srica[n]?|cuban[s]?)\\\\b'\n",
    "regexRace3 = '\\\\b(indian[s]?|chinese|korean|vietnamese|dutch|guamanian[s]?|chamorro|samoan|pacific\\sislander[s]?|han|japanese|filipino[s]?|hawaiian[s]?)\\\\b'\n",
    "\n",
    "regexFull = re.compile('|'.join([regexAge, regexSex, regexRace1, regexRace2, regexRace3]), re.I)\n",
    "\n",
    "#trunk_tables = []\n",
    "def truncate_table(fh, pmcid):\n",
    "    dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "    demographic_table = dynamodb_resource.Table('demographics')\n",
    "    status_table = dynamodb_resource.Table('demographics_meta')\n",
    "    \n",
    "    soup = bs.BeautifulSoup(fh,'lxml')\n",
    "\n",
    "    if soup.find(\"article-title\") is not None:\n",
    "        title = soup.find(\"article-title\").get_text()\n",
    "        demographic_table.update_item(\n",
    "            Key={'pmcid': pmcid},\n",
    "            UpdateExpression=\"set title = :t\",\n",
    "            ExpressionAttributeValues={\n",
    "                ':t': str(title)\n",
    "            },\n",
    "            ReturnValues=\"UPDATED_NEW\"\n",
    "        ) \n",
    "        \n",
    "    table = soup.find(\"table-wrap\")\n",
    "    if table is not None:\n",
    "        tlabel = table.find(\"label\")\n",
    "\n",
    "        if tlabel is not None:\n",
    "            if (len(tlabel.find_all(text=regexInTable)) != 0):\n",
    "                tlabel.string = \"Table 1 Extract\"\n",
    "\n",
    "                if table.attrs['id'] is not None:\n",
    "                    table.attrs.pop('id')\n",
    "\n",
    "                table_id = table.find(\"object-id\")\n",
    "                if table_id is not None:\n",
    "                    table_id.decompose()\n",
    "\n",
    "                tfoot = table.find_all(\"table-wrap-foot\")\n",
    "                if (len(tfoot) != 0):\n",
    "                    [i.decompose() for i in tfoot]\n",
    "\n",
    "                tpermissions = table.find(\"permissions\")\n",
    "                if tpermissions is not None:\n",
    "                    tpermissions.decompose()\n",
    "\n",
    "                tabletables = table.find_all(\"table\")\n",
    "                if (len(tabletables) != 0):\n",
    "                    for tabletable in tabletables:\n",
    "                        tablehasdemo = False\n",
    "                        tbody = tabletable.find(\"tbody\")\n",
    "                        if tbody is not None:\n",
    "                            rows = tbody.find_all(\"tr\")\n",
    "                            bodyhasdemo = False\n",
    "                            hasage = False\n",
    "                            agerow = 0\n",
    "                            for i, row in enumerate(rows):\n",
    "                                firsttd = row.find(\"td\")\n",
    "                                if firsttd is not None:\n",
    "                                    if (len(firsttd.find_all(text=regexFull)) > 0):\n",
    "                                        bodyhasdemo = True\n",
    "                                        if (len(firsttd.find_all(text=re.compile(regexAge, re.I))) > 0):\n",
    "                                            hasage = True\n",
    "                                            agerow = i\n",
    "                                    else:\n",
    "                                        if i > 0:\n",
    "                                            if (hasage and (len(firsttd.find_all(text=regexTime)) > 0) and (agerow == (i - 1)) ):\n",
    "                                                agerow = i\n",
    "                                                bodyhasdemo = True\n",
    "                                            else:\n",
    "                                                row.decompose()\n",
    "                            if bodyhasdemo:\n",
    "                                tablehasdemo = bodyhasdemo\n",
    "                    if tablehasdemo :\n",
    "                        #trunk_tables.append({'Item': {'pmcid': pmcid, 'table': str(table)} })\n",
    "                        demographic_table.update_item(\n",
    "                            Key={'pmcid': pmcid},\n",
    "                            UpdateExpression=\"set table1 = :tab\",\n",
    "                            ExpressionAttributeValues={\n",
    "                                ':tab': str(table)\n",
    "                            },\n",
    "                            ReturnValues=\"UPDATED_NEW\"\n",
    "                        ) \n",
    "                        status_table.update_item(\n",
    "                            Key={\n",
    "                                'source': 'demographics'\n",
    "                            },\n",
    "                            UpdateExpression=\"set with_tables = with_tables + :val\",\n",
    "                            ExpressionAttributeValues={\n",
    "                                ':val': decimal.Decimal(1)\n",
    "                            },\n",
    "                            ReturnValues=\"UPDATED_NEW\"\n",
    "                        )\n",
    "    return \"Success\"\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print('Receiving {} records ...'.format(len(event['Records'])))\n",
    "    for record in event['Records']:\n",
    "        print(record)\n",
    "        if record['eventName'] == 'ObjectCreated:Put':\n",
    "            BUCKET = record['s3']['bucket']['name']\n",
    "            KEY = record['s3']['object']['key']\n",
    "            pmcid = KEY[3:-5]\n",
    "            print(\"Grabbing the file for PMC{} from the S3 bucket\".format(pmcid))\n",
    "            res = boto3.client(\"s3\").get_object(Bucket=BUCKET, Key=KEY)\n",
    "            fh = res[\"Body\"].read().decode('utf-8')\n",
    "            try:\n",
    "                print('Trying table update...')\n",
    "                if not truncate_table(fh, pmcid):\n",
    "                    raise Exception('Table update failed')\n",
    "            except:\n",
    "                print('Table update failed!')\n",
    "                raise\n",
    "            else:\n",
    "                print('Table update passed!')\n",
    "            finally:\n",
    "                print('Table update complete at {}'.format(str(datetime.now())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='stats'></a>\n",
    "\n",
    "UpdateStatsInDemographicsMeta\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Does a final update to the demographics_meta dynamodb table with the statistics of the latest update to the demographics table.   \n",
    "\n",
    "### Associated Role & Policy\n",
    "`PM_lambda_dynamo_S3_role`\n",
    "\n",
    "\n",
    "### trigger:     \n",
    "`TriggerForUpdateStatsWhenSentencesComplete`\n",
    "In S3 bucket pubminer-upload-test\n",
    "arn:aws:s3:::pubminer-upload-test\n",
    "Suffix: .statusEvent type: ObjectCreatedByPut\n",
    "\n",
    "### test event:\n",
    "`TestUpdateStats`  (see testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lambda_statistics_project/lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lambda_statistics_project/lambda_function.py\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "import decimal\n",
    "\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            if o % 1 > 0:\n",
    "                return float(o)\n",
    "            else:\n",
    "                return int(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "\n",
    "dynamodb_resource = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "table = dynamodb_resource.Table('demographics')\n",
    "status_table = dynamodb_resource.Table('demographics_meta')\n",
    "\n",
    "#now = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#date_1 = datetime.strptime(now, \"%Y-%m-%d\")\n",
    "#now = str(now)\n",
    "#since_date = date_1 + timedelta(days=-1)\n",
    "#since_date = str(since_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "def get_latest_stats(pmcids):\n",
    "    numwithsentences = 0 \n",
    "    for dd in pmcids:\n",
    "        response = table.get_item(\n",
    "            Key={\n",
    "                'pmcid': dd\n",
    "            }\n",
    "        )\n",
    "        if 'sentences' in response['Item'].keys():\n",
    "            numwithsentences += 1\n",
    "    return numwithsentences\n",
    "    \n",
    "def puts3item(jsondata):\n",
    "    s3bucketfile = \"update_stats.json\"\n",
    "    s3bucket = \"pubminer-upload\"\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    obj = s3_resource.Object(s3bucket,s3bucketfile)\n",
    "    obj.put(Body=json.dumps(jsondata, cls=DecimalEncoder), ACL='public-read')\n",
    "    return \"Success\"\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print('Receiving {} records ...'.format(len(event['Records'])))\n",
    "    for record in event['Records']:\n",
    "        print(record)\n",
    "        if record['eventName'] == 'ObjectCreated:Put':\n",
    "            BUCKET = record['s3']['bucket']['name']\n",
    "            KEY = record['s3']['object']['key']\n",
    "            if (KEY == 'sentences.status' and BUCKET == 'pubminer-upload'):\n",
    "                print('Updating the update_stats.json...')\n",
    "                try:\n",
    "                    response = status_table.get_item(\n",
    "                        Key={\n",
    "                            'source': 'demographics'\n",
    "                        }\n",
    "                    )\n",
    "                    current_status = response['Item']\n",
    "                    print(\"response\", current_status['pmcids'])\n",
    "                    numwithsentences = get_latest_stats(current_status['pmcids'])\n",
    "                    status_table.update_item(\n",
    "                        Key={\n",
    "                            'source': 'demographics'\n",
    "                        },\n",
    "                        UpdateExpression=\"set with_sentences=:val\",\n",
    "                        ExpressionAttributeValues={\n",
    "                            ':val': decimal.Decimal(numwithsentences)\n",
    "                        },\n",
    "                        ReturnValues=\"UPDATED_NEW\"\n",
    "                    )\n",
    "                    jsondata = {\n",
    "                        \"update\": current_status['date_updated'], \n",
    "                        \"total_items\": current_status['items_total'],\n",
    "                        \"total_updates\": current_status['items_updated'],\n",
    "                        \"with_sentences\": decimal.Decimal(numwithsentences),\n",
    "                        \"with_tables\": current_status['with_tables']\n",
    "                    }\n",
    "                    if not puts3item(jsondata):\n",
    "                        raise Exception('Writing to S3 failed')\n",
    "                except:\n",
    "                    print('UpdateStatsInDemographicsMeta function failed!')\n",
    "                    raise\n",
    "                else:\n",
    "                    print('UpdateStatsInDemographicsMeta function passed!')\n",
    "                    return str(datetime.now())\n",
    "                finally:\n",
    "                    print('UpdateStatsInDemographicsMeta function complete at {}'.format(str(datetime.now())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./sentences_status.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./sentences_status.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3bucketfile = \"sentences.status\"\n",
    "s3bucket = \"pubminer-upload-test\"\n",
    "obj = boto3.resource('s3').Object(s3bucket,s3bucketfile)\n",
    "obj.put(Body=json.dumps({\"update\": str(datetime.now().strftime(\"%Y-%m-%d\"))}), ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='formation'></a>\n",
    "\n",
    "*Cloud Formation template*  \n",
    "=====\n",
    "\n",
    "The Cloud Formation template is created in the cell below. It is then uploaded along with the lambda function packages to S3 from the subsequent cell, and then the Cloud Formation stack is launched using boto3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, get the ARN references for the two DynamoDB streams, which will be used in the template configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_stream_arn = boto3.client('dynamodbstreams').list_streams(TableName='demographics')['Streams'][0]['StreamArn']\n",
    "demographics_meta_stream_arn = boto3.client('dynamodbstreams').list_streams(TableName='demographics_meta')['Streams'][0]['StreamArn']\n",
    "\n",
    "#print(demographics_stream_arn)\n",
    "#print(demographics_meta_stream_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='CFtemplate'></a>\n",
    "\n",
    "Create the template\n",
    "------\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubminer_template_object = {\n",
    "    \"Resources\": {\n",
    "        \"DynamoDBExecutionRole\":{\n",
    "          \"Type\": \"AWS::IAM::Role\",\n",
    "          \"Properties\": {\n",
    "            \"AssumeRolePolicyDocument\": {\n",
    "               \"Version\" : \"2012-10-17\",\n",
    "               \"Statement\": [ {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                     \"Service\": [ \"lambda.amazonaws.com\" ]\n",
    "                  },\n",
    "                  \"Action\": [ \"sts:AssumeRole\" ]\n",
    "               } ]\n",
    "            },\n",
    "            \"Path\": \"/\",\n",
    "            \"Policies\": [ { \n",
    "                \"PolicyName\": \"PM_lambda_dynamodb_execution_policy\",\n",
    "                \"PolicyDocument\": {\n",
    "                    \"Version\": \"2012-10-17\",\n",
    "                    \"Statement\": [\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"lambda:InvokeFunction\",\n",
    "                             \"Resource\":  {\"Fn::Join\": [\":\", [\"arn:aws:lambda:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"function:DownloadPMC-OAFileToS3\"]]}\n",
    "                        },\n",
    "                        {\n",
    "                            \"Sid\": \"AllowAccessToDynamoDBs\",\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"dynamodb:*\",\n",
    "                            \"Resource\": [\n",
    "                                {\"Fn::Join\": [\":\", [\"arn:aws:dynamodb:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"table/demographics\"]]},\n",
    "                                {\"Fn::Join\": [\":\", [\"arn:aws:dynamodb:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"table/demographics_meta\"]]}\n",
    "               ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"logs:CreateLogGroup\",\n",
    "                                \"logs:CreateLogStream\",\n",
    "                                \"logs:PutLogEvents\"\n",
    "                            ],\n",
    "                            \"Resource\": {\"Fn::Join\": [\":\", [\"arn:aws:logs:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"*\"]]}\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"dynamodb:DescribeStream\",\n",
    "                                \"dynamodb:GetRecords\",\n",
    "                                \"dynamodb:GetShardIterator\",\n",
    "                                \"dynamodb:ListStreams\"\n",
    "                            ],\n",
    "                            \"Resource\": demographics_stream_arn\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"s3:GetObject\",\n",
    "                                \"s3:PutObject\",\n",
    "                                \"s3:PutObjectAcl\"\n",
    "                            ],\n",
    "                            \"Resource\": [\n",
    "                                \"arn:aws:s3:::pubminer-upload/*\",\n",
    "                                \"arn:aws:s3:::pubmedcentral_oa/*\"\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }],\n",
    "            \"RoleName\": \"PM_lambda_dynamodb_execution_role\"\n",
    "          }\n",
    "        },\n",
    "        \"DynamoDBS3Role\":{\n",
    "          \"Type\": \"AWS::IAM::Role\",\n",
    "          \"Properties\": {\n",
    "            \"AssumeRolePolicyDocument\": {\n",
    "               \"Version\" : \"2012-10-17\",\n",
    "               \"Statement\": [ {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                     \"Service\": [ \"lambda.amazonaws.com\" ]\n",
    "                  },\n",
    "                  \"Action\": [ \"sts:AssumeRole\" ]\n",
    "               } ]\n",
    "            },\n",
    "            \"Path\": \"/\",\n",
    "            \"Policies\": [ { \n",
    "                    \"PolicyName\": \"PM_lambda_dynamodb_s3_policy\",\n",
    "                    \"PolicyDocument\": {\n",
    "                        \"Version\": \"2012-10-17\",\n",
    "                        \"Statement\": [\n",
    "                            {\n",
    "                                \"Effect\": \"Allow\",\n",
    "                                \"Action\": \"iam:PassRole\",\n",
    "                                \"Resource\": \"*\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Effect\": \"Allow\",\n",
    "                                \"Action\": [\n",
    "                                    \"logs:CreateLogGroup\",\n",
    "                                    \"logs:CreateLogStream\",\n",
    "                                    \"logs:PutLogEvents\"\n",
    "                                ],\n",
    "                                \"Resource\": {\"Fn::Join\": [\":\", [\"arn:aws:logs:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"*\"]]}\n",
    "                            },\n",
    "                            {\n",
    "                                \"Sid\": \"AllowAccessToDynamoDBs\",\n",
    "                                \"Effect\": \"Allow\",\n",
    "                                \"Action\": \"dynamodb:*\",\n",
    "                                \"Resource\": [\n",
    "                                    {\"Fn::Join\": [\":\", [\"arn:aws:dynamodb:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"table/demographics\"]]},\n",
    "                                    {\"Fn::Join\": [\":\", [\"arn:aws:dynamodb:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"table/demographics_meta\"]]}\n",
    "                                ]\n",
    "                            },\n",
    "                            {\n",
    "                                \"Effect\": \"Allow\",\n",
    "                                \"Action\": [\n",
    "                                    \"s3:GetObject\",\n",
    "                                    \"s3:PutObject\",\n",
    "                                    \"s3:PutObjectAcl\"\n",
    "                                ],\n",
    "                                \"Resource\": [\n",
    "                                    \"arn:aws:s3:::pubminer-upload/*\",\n",
    "                                    \"arn:aws:s3:::pubmedcentral_oa/*\"\n",
    "                                ]\n",
    "                            }\n",
    "                        ]\n",
    "                 }\n",
    "            }],\n",
    "            \"RoleName\": \"PM_lambda_dynamodb_s3_role\"\n",
    "          }\n",
    "        },\n",
    "        \"StartStopEC2Role\":{\n",
    "          \"Type\": \"AWS::IAM::Role\",\n",
    "          \"Properties\": {\n",
    "            \"AssumeRolePolicyDocument\": {\n",
    "               \"Version\" : \"2012-10-17\",\n",
    "               \"Statement\": [ {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                     \"Service\": [ \"lambda.amazonaws.com\" ]\n",
    "                  },\n",
    "                  \"Action\": [ \"sts:AssumeRole\" ]\n",
    "               } ]\n",
    "            },\n",
    "            \"Path\": \"/\",\n",
    "            \"Policies\": [ { \n",
    "                \"PolicyName\": \"PM_lambda_start_stop_ec2_policy\",\n",
    "                \"PolicyDocument\": {\n",
    "                    \"Version\": \"2012-10-17\",\n",
    "                    \"Statement\": [\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"iam:PassRole\",\n",
    "                            \"Resource\": \"*\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"iam:ListInstanceProfiles\",\n",
    "                            \"Resource\": \"*\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"lambda:InvokeFunction\",\n",
    "                            \"Resource\": {\"Fn::Join\": [\":\", [\"arn:aws:lambda:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"function:SentenceMinerOnEC2Instance\"]]}\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"logs:CreateLogGroup\",\n",
    "                                \"logs:CreateLogStream\",\n",
    "                                \"logs:PutLogEvents\"\n",
    "                            ],\n",
    "                            \"Resource\": {\"Fn::Join\": [\":\", [\"arn:aws:logs:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"*\"]]}\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"ec2:*\",\n",
    "                            \"Resource\": \"*\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Sid\": \"AllowAccessToPubminerDynamoDBToEC2\",\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": \"dynamodb:*\",\n",
    "                            \"Resource\": {\"Fn::Join\": [\":\", [\"arn:aws:dynamodb:us-east-1\", {\"Ref\":\"AWS::AccountId\"}, \"table/demographics_meta\"]]}\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"dynamodb:DescribeStream\",\n",
    "                                \"dynamodb:GetRecords\",\n",
    "                                \"dynamodb:GetShardIterator\",\n",
    "                                \"dynamodb:ListStreams\"\n",
    "                            ],\n",
    "                            \"Resource\": demographics_meta_stream_arn\n",
    "                        },\n",
    "                        {\n",
    "                            \"Effect\": \"Allow\",\n",
    "                            \"Action\": [\n",
    "                                \"s3:GetObject\",\n",
    "                                \"s3:PutObject\",\n",
    "                                \"s3:PutObjectAcl\"\n",
    "                            ],\n",
    "                            \"Resource\": [\n",
    "                                \"arn:aws:s3:::pubminer-upload/*\",\n",
    "                                \"arn:aws:s3:::pubmedcentral_oa/*\"\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }],\n",
    "            \"RoleName\": \"PM_lambda_start_stop_ec2_role\"\n",
    "          }\n",
    "        },\n",
    "        \"SentenceMinerInstanceRole\": {\n",
    "          \"Type\": \"AWS::IAM::Role\",\n",
    "          \"Properties\": {\n",
    "             \"AssumeRolePolicyDocument\": {\n",
    "                \"Version\" : \"2012-10-17\",\n",
    "                \"Statement\": [ {\n",
    "                   \"Effect\": \"Allow\",\n",
    "                   \"Principal\": {\n",
    "                      \"Service\": [ \"ec2.amazonaws.com\" ]\n",
    "                   },\n",
    "                   \"Action\": [ \"sts:AssumeRole\" ]\n",
    "                } ]\n",
    "             },\n",
    "          }\n",
    "        },\n",
    "        \"SentenceMinerInstanceProfile\":  {\n",
    "           \"Type\": \"AWS::IAM::InstanceProfile\",\n",
    "           \"Properties\": {\n",
    "              \"Path\": \"/\",\n",
    "              \"Roles\": [  { \"Ref\": \"SentenceMinerInstanceRole\" } ],\n",
    "              \"InstanceProfileName\": \"PM_SentenceMinerInstanceProfile\"\n",
    "           }\n",
    "        },\n",
    "        \"GettingUpdates\": {\n",
    "          \"Type\" : \"AWS::Lambda::Function\",\n",
    "          \"Properties\" : {\n",
    "            \"Code\" : {\n",
    "              \"S3Bucket\" : \"pubminer-upload\",\n",
    "              \"S3Key\" : \"lambda_update_project.zip\"\n",
    "            },\n",
    "            \"Description\" : \"Lambda function to update DynamoDB table demographics from PMC-OA\",\n",
    "            \"Environment\" : { \"Variables\" : { \"number_days\":\"2\" } },\n",
    "            \"FunctionName\" : \"GetPMCUpdatesFromCSV\",\n",
    "            \"Handler\" : \"lambda_function.lambda_handler\",\n",
    "            \"MemorySize\" : 2048,\n",
    "            \"Role\" :  { \"Fn::GetAtt\": [\"DynamoDBS3Role\", \"Arn\"] },\n",
    "            \"Runtime\" : \"python3.6\",\n",
    "            \"Timeout\" : 180\n",
    "          }\n",
    "        },\n",
    "        \"DownloadingFiles\": {\n",
    "          \"Type\" : \"AWS::Lambda::Function\",\n",
    "          \"Properties\" : {\n",
    "            \"Code\" : {\n",
    "              \"S3Bucket\" : \"pubminer-upload\",\n",
    "              \"S3Key\" : \"lambda_download_project.zip\"\n",
    "            },\n",
    "            \"Description\" : \"Lambda function download files from PMC-OA and upload to S3\",\n",
    "            \"FunctionName\" : \"DownloadPMC-OAFileToS3\",\n",
    "            \"Handler\" : \"lambda_function.lambda_handler\",\n",
    "            \"MemorySize\" : 128,\n",
    "            \"Role\" : { \"Fn::GetAtt\": [\"DynamoDBExecutionRole\", \"Arn\"] },\n",
    "            \"Runtime\" : \"python3.6\",\n",
    "            \"Timeout\" : 180\n",
    "          }\n",
    "        },        \n",
    "        \"ExtractSentences\": {\n",
    "          \"Type\" : \"AWS::Lambda::Function\",\n",
    "          \"Properties\" : {\n",
    "            \"Code\" : {\n",
    "              \"S3Bucket\" : \"pubminer-upload\",\n",
    "              \"S3Key\" : \"lambda_sentences_project.zip\"\n",
    "            },\n",
    "            \"Description\" : \"Lambda function to launch Sentence Miner EC2 instance\",\n",
    "            \"FunctionName\" : \"SentenceMinerOnEC2Instance\",\n",
    "            \"Handler\" : \"lambda_function.lambda_handler\",\n",
    "            \"MemorySize\" : 128,\n",
    "            \"Role\" :  { \"Fn::GetAtt\": [\"StartStopEC2Role\", \"Arn\"] },\n",
    "            \"Runtime\" : \"python3.6\",\n",
    "            \"Timeout\" : 180\n",
    "          }\n",
    "        },\n",
    "        \"TruncateTables\": {\n",
    "          \"Type\" : \"AWS::Lambda::Function\",\n",
    "          \"Properties\" : {\n",
    "            \"Code\" : {\n",
    "              \"S3Bucket\" : \"pubminer-upload\",\n",
    "              \"S3Key\" : \"lambda_truncate_project.zip\"\n",
    "            },\n",
    "            \"Description\" : \"Lambda function to truncate the tables\",\n",
    "            \"FunctionName\" : \"TruncateTable\",\n",
    "            \"Handler\" : \"lambda_function.lambda_handler\",\n",
    "            \"MemorySize\" : 128,\n",
    "            \"Role\" :  { \"Fn::GetAtt\": [\"DynamoDBS3Role\", \"Arn\"] },\n",
    "            \"Runtime\" : \"python3.6\",\n",
    "            \"Timeout\" : 180\n",
    "          }\n",
    "        },\n",
    "        \"UpdateStatistics\": {\n",
    "          \"Type\" : \"AWS::Lambda::Function\",\n",
    "          \"Properties\" : {\n",
    "            \"Code\" : {\n",
    "              \"S3Bucket\" : \"pubminer-upload\",\n",
    "              \"S3Key\" : \"lambda_statistics_project.zip\"\n",
    "            },\n",
    "            \"Description\" : \"Lambda function to update the statistics on the demographics table\",\n",
    "            \"FunctionName\" : \"UpdateStatsInDemographicsMeta\",\n",
    "            \"Handler\" : \"lambda_function.lambda_handler\",\n",
    "            \"MemorySize\" : 128,\n",
    "            \"Role\" :  { \"Fn::GetAtt\": [\"DynamoDBS3Role\", \"Arn\"] },\n",
    "            \"Runtime\" : \"python3.6\",\n",
    "            \"Timeout\" : 180\n",
    "          }\n",
    "        },\n",
    "        \"ScheduledRule\": {\n",
    "          \"Type\": \"AWS::Events::Rule\",\n",
    "          \"Properties\": {\n",
    "            \"Description\": \"ScheduledRule\",\n",
    "            \"ScheduleExpression\": \"cron(00 6 * * ? *)\",\n",
    "            \"State\": \"ENABLED\",\n",
    "            \"Targets\": [{\n",
    "              \"Arn\": { \"Fn::GetAtt\": [\"GettingUpdates\", \"Arn\"] },\n",
    "              \"Id\": \"TargetFunctionV1\"\n",
    "            }]\n",
    "          }\n",
    "        },\n",
    "        \"PermissionForEventsToInvokeLambda\": {\n",
    "          \"Type\": \"AWS::Lambda::Permission\",\n",
    "          \"Properties\": {\n",
    "            \"FunctionName\": { \"Ref\": \"GettingUpdates\" },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Principal\": \"events.amazonaws.com\",\n",
    "            \"SourceArn\": { \"Fn::GetAtt\": [\"ScheduledRule\", \"Arn\"] }\n",
    "          }\n",
    "        },\n",
    "        \"DownloadingEvent\": {\n",
    "          \"Type\" : \"AWS::Lambda::EventSourceMapping\",\n",
    "          \"Properties\" : {\n",
    "            \"BatchSize\" : 1,\n",
    "            \"Enabled\" : \"true\",\n",
    "            \"EventSourceArn\" : demographics_stream_arn, \n",
    "            \"FunctionName\" : { \"Fn::GetAtt\": [\"DownloadingFiles\", \"Arn\"] },\n",
    "            \"StartingPosition\" : \"LATEST\"\n",
    "          }\n",
    "        },\n",
    "        \"ExtractingEvent\": {\n",
    "          \"Type\" : \"AWS::Lambda::EventSourceMapping\",\n",
    "          \"Properties\" : {\n",
    "            \"BatchSize\" : 1,\n",
    "            \"Enabled\" : \"true\",\n",
    "            \"EventSourceArn\" : demographics_meta_stream_arn, \n",
    "            \"FunctionName\" : { \"Fn::GetAtt\": [\"ExtractSentences\", \"Arn\"] },\n",
    "            \"StartingPosition\" : \"LATEST\"\n",
    "          }\n",
    "        },\n",
    "        \"PermissionForS3UploadToInvokeTruncateLambda\": {\n",
    "          \"Type\": \"AWS::Lambda::Permission\",\n",
    "          \"Properties\": {\n",
    "            \"FunctionName\":  { \"Fn::GetAtt\": [\"TruncateTables\", \"Arn\"] },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Principal\": \"s3.amazonaws.com\",\n",
    "            \"SourceAccount\": { \"Ref\": \"AWS::AccountId\" },\n",
    "            \"SourceArn\": \"arn:aws:s3:::pubmedcentral_oa\"\n",
    "          }\n",
    "        },\n",
    "        \"PermissionForS3UploadToInvokeStatisticsLambda\": {\n",
    "          \"Type\": \"AWS::Lambda::Permission\",\n",
    "          \"Properties\": {\n",
    "            \"FunctionName\": { \"Fn::GetAtt\": [\"UpdateStatistics\", \"Arn\"] },\n",
    "            \"Action\": \"lambda:InvokeFunction\",\n",
    "            \"Principal\": \"s3.amazonaws.com\",\n",
    "            \"SourceAccount\": { \"Ref\": \"AWS::AccountId\" },\n",
    "            \"SourceArn\": \"arn:aws:s3:::pubminer-upload\"\n",
    "          }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the JSON of the template configuration to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_dump = json.dumps(pubminer_template_object)\n",
    "f = open(\"pubminer_template.json\",\"w\")\n",
    "f.write(data_to_dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='uploadLambdas'></a>\n",
    "\n",
    "Upload AWS Lambda packages and template to `pubminer-upload`\n",
    "------\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 upload passed!\n",
      "S3 upload passed!\n",
      "S3 upload passed!\n",
      "S3 upload passed!\n",
      "S3 upload passed!\n",
      "S3 upload passed!\n",
      "S3 upload passed!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "list_of_zip_files = [\"lambda_update_project\", \"lambda_download_project\", \"lambda_sentences_project\", \"lambda_truncate_project\", \"lambda_statistics_project\"]\n",
    "\n",
    "for folder in list_of_zip_files:\n",
    "    shutil.make_archive(folder, 'zip', folder)\n",
    "\n",
    "list_of_zip_files = [ i + \".zip\" for i in list_of_zip_files]\n",
    "files_to_upload = list_of_zip_files + [\"sentences_status.py\", \"PubMiner_s3.jar\"]\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "save_bucket_name = 'pubminer-upload'\n",
    "\n",
    "for filename in files_to_upload:\n",
    "    # upload the file to S3\n",
    "    try:\n",
    "        s3_client.upload_file(filename, save_bucket_name, filename, ExtraArgs={'ACL': 'public-read'})\n",
    "        print('S3 upload passed!')\n",
    "    except:\n",
    "        print('S3 upload failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='launch_stack'></a>\n",
    "\n",
    "Launch the Cloud Formation stack\n",
    "------\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_client = boto3.client('cloudformation')\n",
    "response = cf_client.create_stack(\n",
    "    StackName=\"PubMinerStack\",\n",
    "    TemplateBody=json.dumps(pubminer_template_object),\n",
    "    DisableRollback=False,\n",
    "    TimeoutInMinutes=10,\n",
    "    Capabilities = ['CAPABILITY_IAM', 'CAPABILITY_NAMED_IAM'],\n",
    "    Tags=[{'Key': 'Name', 'Value': 'PubMiner CF Stack'}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the event notification configuration to the two S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncateTableARN = boto3.client('lambda').get_function_configuration( FunctionName='TruncateTable' )['FunctionArn']\n",
    "updateStatsARN = boto3.client('lambda').get_function_configuration( FunctionName='UpdateStatsInDemographicsMeta' )['FunctionArn']\n",
    "\n",
    "#print(truncateTableARN)\n",
    "#print(updateStatsARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, for the TruncateTable Lambda function triggered by uploads to `pubmedcentral_oa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket notification updated successfully\n"
     ]
    }
   ],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "config_data = {         \n",
    "    'LambdaFunctionConfigurations': [\n",
    "        {\n",
    "            'Id': 'TriggerTruncateTableEventOnUpload',\n",
    "            'LambdaFunctionArn': truncateTableARN,\n",
    "            'Events': [\n",
    "                's3:ObjectCreated:*',\n",
    "            ],\n",
    "            'Filter': {\n",
    "                'Key': {\n",
    "                    'FilterRules': [\n",
    "                        {\n",
    "                            'Name': 'suffix',\n",
    "                            'Value': '.nxml'\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]}\n",
    "bucket_notification = s3_resource.BucketNotification('pubmedcentral_oa')\n",
    "response = bucket_notification.put(NotificationConfiguration=config_data)\n",
    "print('Bucket notification updated successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, for the Lambda function for updating of statistics, triggered by completion of sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket notification updated successfully\n"
     ]
    }
   ],
   "source": [
    "config_data = {         \n",
    "    'LambdaFunctionConfigurations': [\n",
    "        {\n",
    "            'Id': 'TriggerUpdateStatsWhenSentencesComplete',\n",
    "            'LambdaFunctionArn': updateStatsARN,\n",
    "            'Events': [\n",
    "                's3:ObjectCreated:Put',\n",
    "            ],\n",
    "            'Filter': {\n",
    "                'Key': {\n",
    "                    'FilterRules': [\n",
    "                        {\n",
    "                            'Name': 'suffix',\n",
    "                            'Value': '.status'\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]}\n",
    "bucket_notification = s3_resource.BucketNotification('pubminer-upload')\n",
    "response = bucket_notification.put(NotificationConfiguration=config_data)\n",
    "print('Bucket notification updated successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='tear_down_stack'></a>\n",
    "\n",
    "Tear down the Cloud Formation stack\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "** This will remove the PubMiner backend system of AWS Lambda functions **    \n",
    "\n",
    "Do this only when wanting to redo the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_client = boto3.client('cloudformation')\n",
    "response = cf_client.delete_stack( StackName=\"PubMinerStack\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='testing'></a>\n",
    "\n",
    "*Testing the installation*  \n",
    "=====\n",
    "\n",
    "The following cells contain the test examples for the Lambda functions and the test suite for the BeautifulSoup parsing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='Testlambda'></a>\n",
    "\n",
    "Test events for AWS Lambda functions\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "The next cells contains the JSON code for the test events of the individual Lambda functions. They can each be copy-pasted into the Test Event boxes in AWS to test the Lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla  test event for  `GetPMCUpdatesFromCSV`\n",
    "\n",
    "Triggers the entire update workflow from a vanilla test event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FailedEntryCount': 0, 'Entries': [{'EventId': '97f7e81f-d74b-c1cd-9def-0890c97e7125'}], 'ResponseMetadata': {'RequestId': '274b9b36-5485-11e8-8327-413b3b94c1b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '274b9b36-5485-11e8-8327-413b3b94c1b0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 10 May 2018 19:05:49 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Create CloudWatchEvents client\n",
    "cloudwatch_events = boto3.client('events')\n",
    "\n",
    "# Get the ARN of the function\n",
    "getUpdatesARN = boto3.client('lambda').get_function_configuration( FunctionName='GetPMCUpdatesFromCSV' )['FunctionArn']\n",
    "\n",
    "# Put an event\n",
    "response = cloudwatch_events.put_events(\n",
    "    Entries=[\n",
    "        {\n",
    "            'Detail': json.dumps({'key1': 'value1', 'key2': 'value2'}),\n",
    "            'DetailType': 'appRequestSubmitted',\n",
    "            'Resources': [\n",
    "                getUpdatesARN,\n",
    "            ],\n",
    "            'Source': 'Trigger'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the Lambda function with the test event and check the tail of the log for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-10 19:06:07.577959\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=getUpdatesARN,\n",
    "    InvocationType='RequestResponse',\n",
    "    LogType='Tail',\n",
    "    Payload=json.dumps({'key1': 'value1', 'key2': 'value2'})\n",
    ")\n",
    "\n",
    "print(json.loads(response['Payload'].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpdateToDynamoDB  test event for  `DownloadPMC-OAFileToS3`\n",
    "\n",
    "Includes 3 files that are in the OA subset and 1 file (the last) that is not. Test the downloading and the increment and decrement/deletion operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "UpdateToDynamoDBtestevent = {\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"eventID\": \"1\",\n",
    "      \"eventVersion\": \"1.0\",\n",
    "      \"dynamodb\": {\n",
    "        \"Keys\": {\n",
    "          \"pmcid\": {\n",
    "            \"S\": \"5920940\"\n",
    "          }\n",
    "        },\n",
    "        \"StreamViewType\": \"KEYS\",\n",
    "        \"SequenceNumber\": \"111\",\n",
    "        \"SizeBytes\": 26\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"INSERT\",\n",
    "      \"eventSourceARN\": demographics_stream_arn,\n",
    "      \"eventSource\": \"aws:dynamodb\"\n",
    "    },\n",
    "    {\n",
    "      \"eventID\": \"2\",\n",
    "      \"eventVersion\": \"1.0\",\n",
    "      \"dynamodb\": {\n",
    "        \"Keys\": {\n",
    "          \"pmcid\": {\n",
    "            \"S\": \"5935860\"\n",
    "          }\n",
    "        },\n",
    "        \"StreamViewType\": \"KEYS\",\n",
    "        \"SequenceNumber\": \"111\",\n",
    "        \"SizeBytes\": 26\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"INSERT\",\n",
    "      \"eventSourceARN\": demographics_stream_arn,\n",
    "      \"eventSource\": \"aws:dynamodb\"\n",
    "    },\n",
    "    {\n",
    "      \"eventID\": \"3\",\n",
    "      \"eventVersion\": \"1.0\",\n",
    "      \"dynamodb\": {\n",
    "        \"Keys\": {\n",
    "          \"pmcid\": {\n",
    "            \"S\": \"5916111\"\n",
    "          }\n",
    "        },\n",
    "        \"StreamViewType\": \"KEYS\",\n",
    "        \"SequenceNumber\": \"111\",\n",
    "        \"SizeBytes\": 26\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"INSERT\",\n",
    "      \"eventSourceARN\": demographics_stream_arn,\n",
    "      \"eventSource\": \"aws:dynamodb\"\n",
    "    },\n",
    "    {\n",
    "      \"eventID\": \"4\",\n",
    "      \"eventVersion\": \"1.0\",\n",
    "      \"dynamodb\": {\n",
    "        \"Keys\": {\n",
    "          \"pmcid\": {\n",
    "            \"S\": \"2842548\"\n",
    "          }\n",
    "        },\n",
    "        \"StreamViewType\": \"KEYS\",\n",
    "        \"SequenceNumber\": \"111\",\n",
    "        \"SizeBytes\": 26\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"INSERT\",\n",
    "      \"eventSourceARN\": demographics_stream_arn,\n",
    "      \"eventSource\": \"aws:dynamodb\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FailedEntryCount': 0, 'Entries': [{'EventId': 'f6fd6a01-223e-6164-d0e7-3a1f987b7343'}], 'ResponseMetadata': {'RequestId': '314bec3a-5486-11e8-a85e-b3d04ecf237e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '314bec3a-5486-11e8-a85e-b3d04ecf237e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 10 May 2018 19:13:15 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Get the ARN of the function\n",
    "downloadPMCtoS3ARN = boto3.client('lambda').get_function_configuration( FunctionName='DownloadPMC-OAFileToS3' )['FunctionArn']\n",
    "\n",
    "# Put an event\n",
    "response = cloudwatch_events.put_events(\n",
    "    Entries=[\n",
    "        {\n",
    "            'Detail': json.dumps(UpdateToDynamoDBtestevent),\n",
    "            'DetailType': 'UpdateToDynamoDB',\n",
    "            'Resources': [\n",
    "                downloadPMCtoS3ARN,\n",
    "            ],\n",
    "            'Source': 'Trigger'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the Lambda function with the test event and check the tail of the log for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=downloadPMCtoS3ARN,\n",
    "    InvocationType='RequestResponse',\n",
    "    LogType='Tail',\n",
    "    Payload=json.dumps(UpdateToDynamoDBtestevent)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Payload'].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynamoTriggerExtraction  test event for  `SentenceMinerOnEC2Instance`\n",
    "\n",
    "Provides the event that `items_downloaded` increases and reaches the total number of `items_updated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DynamoTriggerExtractiontestevent = {\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"eventID\": \"1\",\n",
    "      \"eventVersion\": \"1.0\",\n",
    "      \"dynamodb\": {\n",
    "        \"OldImage\": {\n",
    "          \"items_downloaded\": {\n",
    "            \"N\": 99\n",
    "          },\n",
    "          \"items_updated\": {\n",
    "            \"N\": 100\n",
    "          }\n",
    "        },\n",
    "        \"SequenceNumber\": \"222\",\n",
    "        \"Keys\": {\n",
    "          \"source\": {\n",
    "            \"S\": \"demographics\"\n",
    "          }\n",
    "        },\n",
    "        \"SizeBytes\": 59,\n",
    "        \"NewImage\": {\n",
    "          \"items_downloaded\": {\n",
    "            \"N\": 100\n",
    "          },\n",
    "          \"items_updated\": {\n",
    "            \"N\": 100\n",
    "          }\n",
    "        },\n",
    "        \"StreamViewType\": \"NEW_AND_OLD_IMAGES\"\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"MODIFY\",\n",
    "      \"eventSourceARN\": demographics_meta_stream_arn,\n",
    "      \"eventSource\": \"aws:dynamodb\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FailedEntryCount': 0, 'Entries': [{'EventId': '55d8df5d-a66c-72bb-06c1-de52902ccbe3'}], 'ResponseMetadata': {'RequestId': '395da836-5486-11e8-a395-e1c090485db8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '395da836-5486-11e8-a395-e1c090485db8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 10 May 2018 19:13:29 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "sentenceMinerARN = boto3.client('lambda').get_function_configuration( FunctionName='SentenceMinerOnEC2Instance' )['FunctionArn']\n",
    "response = cloudwatch_events.put_events(\n",
    "    Entries=[\n",
    "        {\n",
    "            'Detail': json.dumps(DynamoTriggerExtractiontestevent),\n",
    "            'DetailType': 'DynamoTriggerExtraction',\n",
    "            'Resources': [\n",
    "                sentenceMinerARN,\n",
    "            ],\n",
    "            'Source': 'Trigger'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the Lambda function with the test event and check the tail of the log for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-10 19:13:33.819428\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=sentenceMinerARN,\n",
    "    InvocationType='RequestResponse',\n",
    "    LogType='Tail',\n",
    "    Payload=json.dumps(DynamoTriggerExtractiontestevent)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Payload'].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 upload  test event for  `TruncateTable`\n",
    "\n",
    "Provides the event that an XML file of an article is uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Uploadtestevent = {\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"s3\": {\n",
    "        \"object\": {\n",
    "          \"key\": \"PMC5935858.nxml\"\n",
    "        },\n",
    "        \"bucket\": {\n",
    "          \"arn\": \"arn:aws:s3:::pubmedcentral_oa\",\n",
    "          \"name\": \"pubmedcentral_oa\"\n",
    "        },\n",
    "        \"s3SchemaVersion\": \"1.0\"\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"eventSource\": \"aws:s3\"\n",
    "    },\n",
    "    {\n",
    "      \"s3\": {\n",
    "        \"object\": {\n",
    "          \"key\": \"PMC5934515.nxml\"\n",
    "        },\n",
    "        \"bucket\": {\n",
    "          \"arn\": \"arn:aws:s3:::pubmedcentral_oa\",\n",
    "          \"name\": \"pubmedcentral_oa\"\n",
    "        },\n",
    "        \"s3SchemaVersion\": \"1.0\"\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"eventSource\": \"aws:s3\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FailedEntryCount': 0, 'Entries': [{'EventId': '9dcba488-2ab8-cf49-05db-11490ad83ea7'}], 'ResponseMetadata': {'RequestId': '509a2484-5486-11e8-a395-e1c090485db8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '509a2484-5486-11e8-a395-e1c090485db8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 10 May 2018 19:14:08 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "truncateTableARN = boto3.client('lambda').get_function_configuration( FunctionName='TruncateTable' )['FunctionArn']\n",
    "response = cloudwatch_events.put_events(\n",
    "    Entries=[\n",
    "        {\n",
    "            'Detail': json.dumps(S3Uploadtestevent),\n",
    "            'DetailType': 'TruncateTable',\n",
    "            'Resources': [\n",
    "                truncateTableARN,\n",
    "            ],\n",
    "            'Source': 'Trigger'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the Lambda function with the test event and check the tail of the log for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=truncateTableARN,\n",
    "    InvocationType='RequestResponse',\n",
    "    LogType='Tail',\n",
    "    Payload=json.dumps(S3Uploadtestevent)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Payload'].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 upload test event for  `UpdateStatsInDemographicsMeta`\n",
    "\n",
    "Provides the event that the sentence.status file from the SentenceMiner instance is uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "UpdateStatstestevent = {\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"s3\": {\n",
    "        \"object\": {\n",
    "          \"key\": \"sentences.status\"\n",
    "        },\n",
    "        \"bucket\": {\n",
    "          \"arn\": \"arn:aws:s3:::pubminer\",\n",
    "          \"name\": \"pubminer-upload\"\n",
    "        },\n",
    "        \"s3SchemaVersion\": \"1.0\"\n",
    "      },\n",
    "      \"awsRegion\": \"us-east-1\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"eventSource\": \"aws:s3\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FailedEntryCount': 0, 'Entries': [{'EventId': 'dd0b5229-9a62-19d6-6173-983c0dbf800e'}], 'ResponseMetadata': {'RequestId': '60453812-5486-11e8-8dca-6156cc8dd389', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '60453812-5486-11e8-8dca-6156cc8dd389', 'content-type': 'application/x-amz-json-1.1', 'content-length': '85', 'date': 'Thu, 10 May 2018 19:14:35 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "updateStatsARN = boto3.client('lambda').get_function_configuration( FunctionName='UpdateStatsInDemographicsMeta' )['FunctionArn']\n",
    "response = cloudwatch_events.put_events(\n",
    "    Entries=[\n",
    "        {\n",
    "            'Detail': json.dumps(UpdateStatstestevent),\n",
    "            'DetailType': 'UpdateStats',\n",
    "            'Resources': [\n",
    "                updateStatsARN,\n",
    "            ],\n",
    "            'Source': 'Trigger'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the Lambda function with the test event and check the tail of the log for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-10 19:14:53.722902\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=getUpdatesARN,\n",
    "    InvocationType='RequestResponse',\n",
    "    LogType='Tail',\n",
    "    Payload=json.dumps({'key1': 'value1', 'key2': 'value2'})\n",
    ")\n",
    "\n",
    "print(json.loads(response['Payload'].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back-to-top](#back-to-top)\n",
    "<a id='crummytest'></a>\n",
    "\n",
    "BeautifulSoup tests\n",
    "------\n",
    "\n",
    "----\n",
    "\n",
    "The set of BeautifulSoup tests can be run from the notebook to confirm that the table mining functions are working as expected. The tests are based on the installation unittests of BeautifulSoup3. The source tests can be found [here.](https://www.crummy.com/software/BeautifulSoup/bs3/download/3.x/BeautifulSoup-3.0.8/BeautifulSoupTests.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/anaconda3/lib/python3.6/site-packages/bs4/builder/_lxml.py:250: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  self.parser.feed(markup)\n",
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.036s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "#%%writefile PubMinerSoupTests.py\n",
    "import unittest\n",
    "import re\n",
    "from bs4 import *\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "'''\n",
    "Tests for BeautifulSoup based on the package tests using Python unittest \n",
    "found here : https://www.crummy.com/software/BeautifulSoup/bs3/download/3.x/BeautifulSoup-3.0.8/BeautifulSoupTests.py\n",
    "'''\n",
    "\n",
    "class SoupTest(unittest.TestCase):\n",
    "\n",
    "    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):\n",
    "        \"\"\"Parse the given text and make sure its string rep is the other\n",
    "        given text.\"\"\"\n",
    "        if rep == None:\n",
    "            rep = toParse\n",
    "        self.assertEqual(str(c(toParse)), rep)\n",
    "\n",
    "\n",
    "class FollowThatTag(SoupTest):\n",
    "\n",
    "    \"Tests the various ways of fetching tags from a soup.\"\n",
    "\n",
    "    def setUp(self):\n",
    "        ml = \"\"\"\n",
    "        <front>\n",
    "            <article-meta>\n",
    "                <title-group>\n",
    "                    <article-title>A good title</article-title>\n",
    "                </title-group>\n",
    "            </article-meta>\n",
    "        </front>\n",
    "        <table-wrap id=\"T1\" orientation=\"portrait\" position=\"float\">\n",
    "            <label>Table 1.</label>\n",
    "            <caption>\n",
    "                <p>Demographics</p>\n",
    "            </caption>\n",
    "            <table frame=\"vsides\" rules=\"groups\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th align=\"left\" valign=\"bottom\" rowspan=\"1\" colspan=\"1\"/>\n",
    "                        <th align=\"center\" valign=\"bottom\" rowspan=\"1\" colspan=\"1\">Sham (n=11)</th>\n",
    "                        <th align=\"center\" valign=\"bottom\" rowspan=\"1\" colspan=\"1\">ECT (n=14)</th>\n",
    "                        <th align=\"center\" valign=\"bottom\" rowspan=\"1\" colspan=\"1\"><italic>P</italic> value</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">Gender, n/% males</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">7/63</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">8/53</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.7</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">Age, median (IQR)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">37 (32)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">40 (29)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.9</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">Age at onset, median (IQR)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">22 (24)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">18 (30)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.5</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">First hospitalization, median (IQR)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">24 (30)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">39 (30)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.9</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">Number of episodes, median (IQR)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">5 (8)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">9 (20)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.6</td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td rowspan=\"1\" colspan=\"1\">Education (yrs.), mean (SD)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">16 (2)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">14 (2)</td>\n",
    "                        <td align=\"center\" rowspan=\"1\" colspan=\"1\">.04</td>\n",
    "                    </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "            <table>This is another table.</table>\n",
    "            <table-wrap-foot>\n",
    "                <fn id=\"fn-01\">\n",
    "                    <p>Abbreviations: SD, standard deviation; STAI, Yrs, years.</p>\n",
    "                </fn>\n",
    "            </table-wrap-foot>\n",
    "            <table-wrap-foot>\n",
    "                <fn id=\"fn-02\">\n",
    "                    <p>Another great table footer.</p>\n",
    "                </fn>\n",
    "            </table-wrap-foot>\n",
    "            <permissions>\n",
    "                <license license-type=\"cc-by-nc\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "                    <license-p>This is an Open Access article</license-p>\n",
    "                    <license-q></license-q>\n",
    "                </license>\n",
    "            </permissions>\"\"\"\n",
    "        self.soup = BeautifulSoup(ml,'lxml')\n",
    "\n",
    "    def testFindAllByName(self):\n",
    "        # Run tests on finding the elements used in the table truncation\n",
    "        matching = self.soup('article-title')\n",
    "        self.assertEqual(len(matching), 1)\n",
    "        self.assertEqual(matching[0].name, 'article-title')\n",
    "        self.assertEqual(matching, self.soup.find_all('article-title'))\n",
    "\n",
    "        matching2 = self.soup('table-wrap')\n",
    "        self.assertEqual(len(matching2), 1)\n",
    "        self.assertEqual(matching2[0].name, 'table-wrap')\n",
    "        self.assertEqual(matching2, self.soup.find_all('table-wrap'))\n",
    "\n",
    "        matching3 = self.soup('label')\n",
    "        self.assertEqual(len(matching3), 1)\n",
    "        self.assertEqual(matching3[0].name, 'label')\n",
    "        self.assertEqual(matching3, self.soup.find_all('label'))\n",
    "\n",
    "        matching4 = self.soup('table')\n",
    "        self.assertEqual(len(matching4), 2)\n",
    "        self.assertEqual(matching4[0].name, 'table')\n",
    "        self.assertEqual(matching4[1].name, 'table')\n",
    "        self.assertEqual(matching4, self.soup.find_all('table'))\n",
    "\n",
    "        matching5 = self.soup('table-wrap-foot')\n",
    "        self.assertEqual(len(matching5), 2)\n",
    "        self.assertEqual(matching5[0].name, 'table-wrap-foot')\n",
    "        self.assertEqual(matching5[1].name, 'table-wrap-foot')\n",
    "        self.assertEqual(matching5, self.soup.find_all('table-wrap-foot'))        \n",
    "\n",
    "        matching6 = self.soup('permissions')\n",
    "        self.assertEqual(len(matching6), 1)\n",
    "        self.assertEqual(matching6[0].name, 'permissions')\n",
    "        self.assertEqual(matching6, self.soup.find_all('permissions'))    \n",
    "        \n",
    "        matching7 = self.soup('tbody')\n",
    "        self.assertEqual(len(matching7), 1)\n",
    "        self.assertEqual(matching7[0].name, 'tbody')\n",
    "        self.assertEqual(matching7, self.soup.find_all('tbody'))\n",
    "        \n",
    "        matching8 = self.soup('tr')\n",
    "        self.assertEqual(len(matching8), 7)\n",
    "        self.assertEqual(matching8[0].name, 'tr')\n",
    "        self.assertEqual(matching8, self.soup.find_all('tr'))\n",
    "        \n",
    "    def testFindAllByAttribute(self):\n",
    "        # Run tests on finding some elements by attribute\n",
    "        matching = self.soup.findAll(id='T1')\n",
    "        self.assertEqual(len(matching), 1)\n",
    "        self.assertEqual(matching[0].name, 'table-wrap')\n",
    "\n",
    "        matching2 = self.soup.findAll(attrs={'id' : 'T1'})\n",
    "        self.assertEqual(matching, matching2)\n",
    "        \n",
    "        matching3 = self.soup.findAll(id='fn-01')\n",
    "        self.assertEqual(len(matching3), 1)\n",
    "        self.assertEqual(matching3[0].name, 'fn')\n",
    "        \n",
    "        matching4 = self.soup.findAll(attrs={'license-type' : 'cc-by-nc'})\n",
    "        self.assertEqual(len(matching4), 1)\n",
    "        self.assertEqual(matching4[0].name, 'license')\n",
    "        \n",
    "        self.assertEqual(len(self.soup.find_all(id=None)), 57)\n",
    "\n",
    "        self.assertEqual(len(self.soup.find_all(rowspan=\"1\")), 28)\n",
    "        self.assertEqual(len(self.soup.find_all(colspan=\"1\")), 28)\n",
    "        self.assertEqual(len(self.soup.find_all(align=\"center\")), 21)\n",
    "        self.assertEqual(len(self.soup.find_all(junk=None)), 60)\n",
    "        self.assertEqual(len(self.soup.find_all(junk=[1, None])), 0)\n",
    "\n",
    "        self.assertEqual(len(self.soup.find_all(junk=re.compile('.*'))), 0)\n",
    "        self.assertEqual(len(self.soup.find_all(junk=True)), 0)\n",
    "\n",
    "        self.assertEqual(len(self.soup.find_all(junk=True)), 0)\n",
    "        self.assertEqual(len(self.soup.find_all(href=True)), 1)     \n",
    "\n",
    "    def testFindAllByList(self):\n",
    "        matching = self.soup(['table-wrap', 'label', 'table', 'tbody', 'table-wrap-foot'])\n",
    "        self.assertEqual(len(matching), 7)\n",
    "        \n",
    "        \n",
    "    def testFindAllText(self):\n",
    "        soup = BeautifulSoup(\"<html>\\xbb</html>\", 'lxml')\n",
    "        self.assertEqual(soup.findAll(text=re.compile('.*')),\n",
    "                         [u'\\xbb'])\n",
    "        \n",
    "        matching = self.soup('article-title')\n",
    "        self.assertEqual(len(matching), 1)\n",
    "        self.assertEqual(matching[0].get_text(), 'A good title')\n",
    "        \n",
    "    def testFindAllByRE(self):\n",
    "        regexTable1inTable = '(tab(:?.|le|le.|le-|leau|ela)? *?(1[.]?|I[.]?)$)'\n",
    "        regexInTable = re.compile(regexTable1inTable, re.I)\n",
    "        self.assertEqual(len(self.soup(text=regexInTable)), 1)\n",
    "\n",
    "        regexAge = '\\\\b(age[d|s]?)\\\\b'\n",
    "        regexTime1 = '(\\\\S+\\\\s*\\\\d+(.\\\\d+)?)$'\n",
    "        regexTime2 = '\\\\b(year[s]?|week[s]?|month[s]?|mean|(max|min)(imum)?)\\\\b'\n",
    "        regexTime = re.compile('|'.join([regexTime1, regexTime2]), re.I)\n",
    "        self.assertEqual(len(self.soup(text=regexTime)), 10)\n",
    "\n",
    "        regexSex = '\\\\b(sex(es)?|gender[s]?|male[s]?|female[s]?|(wo)?men[s]?|boy[s]?|girl[s]?)\\\\b'\n",
    "        regexSex = re.compile(regexSex, re.I)\n",
    "        self.assertEqual(len(self.soup(text=regexSex)), 1)\n",
    "        \n",
    "\n",
    "    def testLen(self):\n",
    "        matching = self.soup('table')\n",
    "        self.assertEqual(len(matching[0].tbody), 13)\n",
    "        \n",
    "        matching2 = self.soup('permissions')\n",
    "        self.assertEqual(len(matching2[0].license), 5)\n",
    "\n",
    "    def testString(self):\n",
    "        matching = self.soup('label')\n",
    "        self.assertEqual(matching[0].string, 'Table 1.')\n",
    "\n",
    "    def testLackOfString(self):\n",
    "        matching = self.soup('license-q')\n",
    "        self.assertTrue(not matching[0].string)\n",
    "\n",
    "    def testStringAssign(self):\n",
    "        matching = self.soup('label')\n",
    "        matching[0].string = 'Table 1.'\n",
    "        string = matching[0].string\n",
    "        self.assertEqual(string, 'Table 1.')\n",
    "\n",
    "    def testText(self):\n",
    "        matching = self.soup('table-wrap')\n",
    "        self.assertEqual(matching[0].label.getText(), 'Table 1.')\n",
    "\n",
    "    \n",
    "    def testTagReplacement(self):\n",
    "        # Make sure you can replace an element with itself.\n",
    "        matching = self.soup.findAll(id='T1')\n",
    "        label = matching[0].label\n",
    "        matching[0].label.replaceWith(label)\n",
    "        self.assertEqual(matching[0].label,  self.soup('label')[0])\n",
    "        \n",
    "        \n",
    "    def testDecompose(self):\n",
    "        matching = self.soup\n",
    "        self.assertEqual(len(matching('table')[0].tbody), 13)\n",
    "        regexSex = '\\\\b(sex(es)?|gender[s]?|male[s]?|female[s]?|(wo)?men[s]?|boy[s]?|girl[s]?)\\\\b'\n",
    "        regexSex = re.compile(regexSex, re.I)\n",
    "        tbody = matching.find(\"tbody\")\n",
    "        tbody.find_all(text=regexSex)[0].parent.parent.decompose()\n",
    "        self.assertEqual(len(matching('table')[0].tbody), 12)\n",
    "        \n",
    "        matching2 = self.soup('table-wrap-foot')\n",
    "        self.assertEqual(len(matching2), 2)\n",
    "        matching2 = self.soup.find_all(attrs={'id' : 'fn-02'})\n",
    "        matching2[0].parent.decompose()\n",
    "        matching2 = self.soup('table-wrap-foot')\n",
    "        self.assertEqual(len(matching2), 1)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the database tables\n",
    "--------\n",
    "\n",
    "\n",
    "**Warning: Do this only to tear down the system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table\n"
     ]
    }
   ],
   "source": [
    "dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')\n",
    "\n",
    "try:\n",
    "    dynamodb_client.delete_table(TableName='demographics')\n",
    "    print('Deleted table')\n",
    "        \n",
    "except dynamodb_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Table does not exist - cannot delete it\")\n",
    "    pass\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "except ValueError:\n",
    "    print(\"Could not convert data to an integer.\")\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table\n"
     ]
    }
   ],
   "source": [
    "dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')\n",
    "\n",
    "try:\n",
    "    dynamodb_client.delete_table(TableName='demographics_meta')\n",
    "    print('Deleted table')\n",
    "        \n",
    "except dynamodb_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Table does not exist - cannot delete it\")\n",
    "    pass\n",
    "except OSError as err:\n",
    "    print(\"OS error: {0}\".format(err))\n",
    "except ValueError:\n",
    "    print(\"Could not convert data to an integer.\")\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
